{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyter_contrib_nbextensions && jupyter contrib nbextension install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  음료수 캔 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 키워드 입력\n",
    "keyword = \"음료수 캔\"\n",
    "key = \"soda can\"\n",
    "\n",
    "\n",
    "\n",
    "# 웹접속 - 네이버 이미지 접속\n",
    "# 79.0.3945.36 / chrome version\n",
    "print(\"접속중\")\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver.exe\")\n",
    "driver.implicitly_wait(30)\n",
    "\n",
    "url = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query={}'.format(keyword)\n",
    "driver.get(url)\n",
    "# 페이지 스크롤 다운\n",
    "body = driver.find_element_by_css_selector('body')\n",
    "for i in range(60):\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "\n",
    "# 이미지 링크 수집\n",
    "imgs = driver.find_elements_by_css_selector('img._img')\n",
    "result = []\n",
    "for img in tqdm(imgs):\n",
    "    if 'http' in img.get_attribute('src'):\n",
    "        result.append(img.get_attribute('src'))\n",
    "# print(result)\n",
    "\n",
    "driver.close()\n",
    "print('수집완료')\n",
    "\n",
    "# 파일 저장\n",
    "import os\n",
    "if not os.path.isdir('./{}'.format(key)):\n",
    "    print('폴더생성')\n",
    "    os.mkdir('./{}'.format(key))\n",
    "\n",
    "# 다운로드\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "print(\"다운로드중\")\n",
    "for index, link in tqdm(enumerate(result)):\n",
    "    start = link.rfind('.')\n",
    "    end = link.rfind('&')\n",
    "    # print(link[start:end])\n",
    "    filetype = link[start:end]\n",
    "    if filetype == '.jpg':  # jpg 만 다운\n",
    "        urlretrieve(link, './{}/{}{}{}'.format(key, key, index, filetype))\n",
    "        time.sleep(1)\n",
    "print(\"다운로드 완료\")\n",
    "\n",
    "# import zipfile\n",
    "# zip_file = zipfile.Zipfile('./{}.zip'.format(keyword),'w')\n",
    "# # 이 폴더안에 파일 가져오는거\n",
    "# for image in tqdm(os.listdir('./{}'.format(keyword))):\n",
    "#     zip_file.write('./{}/{}'.format(keyword,image), compress_type=zip_file.ZIP_DEFLATED)\n",
    "# zip_file.close()\n",
    "# print(\"압축완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통조림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 키워드 입력\n",
    "keyword = \"통조림 캔\"\n",
    "key = \"canned\"\n",
    "\n",
    "\n",
    "\n",
    "# 웹접속 - 네이버 이미지 접속\n",
    "# 79.0.3945.36 / chrome version\n",
    "print(\"접속중\")\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver.exe\")\n",
    "driver.implicitly_wait(30)\n",
    "\n",
    "url = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query={}'.format(keyword)\n",
    "driver.get(url)\n",
    "# 페이지 스크롤 다운\n",
    "body = driver.find_element_by_css_selector('body')\n",
    "for i in range(60):\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "\n",
    "# 이미지 링크 수집\n",
    "imgs = driver.find_elements_by_css_selector('img._img')\n",
    "result = []\n",
    "for img in tqdm(imgs):\n",
    "    if 'http' in img.get_attribute('src'):\n",
    "        result.append(img.get_attribute('src'))\n",
    "# print(result)\n",
    "\n",
    "driver.close()\n",
    "print('수집완료')\n",
    "\n",
    "# 파일 저장\n",
    "import os\n",
    "if not os.path.isdir('./{}'.format(key)):\n",
    "    print('폴더생성')\n",
    "    os.mkdir('./{}'.format(key))\n",
    "\n",
    "# 다운로드\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "print(\"다운로드중\")\n",
    "for index, link in tqdm(enumerate(result)):\n",
    "    start = link.rfind('.')\n",
    "    end = link.rfind('&')\n",
    "    # print(link[start:end])\n",
    "    filetype = link[start:end]\n",
    "    if filetype == '.jpg':  # jpg 만 다운\n",
    "        urlretrieve(link, './{}/{}{}{}'.format(key, key, index, filetype))\n",
    "        time.sleep(1)\n",
    "print(\"다운로드 완료\")\n",
    "\n",
    "# import zipfile\n",
    "# zip_file = zipfile.Zipfile('./{}.zip'.format(keyword),'w')\n",
    "# # 이 폴더안에 파일 가져오는거\n",
    "# for image in tqdm(os.listdir('./{}'.format(keyword))):\n",
    "#     zip_file.write('./{}/{}'.format(keyword,image), compress_type=zip_file.ZIP_DEFLATED)\n",
    "# zip_file.close()\n",
    "# print(\"압축완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 캔맥주"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 키워드 입력\n",
    "keyword = \"캔맥주\"\n",
    "key = \"beer\"\n",
    "\n",
    "\n",
    "\n",
    "# 웹접속 - 네이버 이미지 접속\n",
    "# 79.0.3945.36 / chrome version\n",
    "print(\"접속중\")\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver.exe\")\n",
    "driver.implicitly_wait(30)\n",
    "\n",
    "url = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query={}'.format(keyword)\n",
    "driver.get(url)\n",
    "# 페이지 스크롤 다운\n",
    "body = driver.find_element_by_css_selector('body')\n",
    "for i in range(60):\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "\n",
    "# 이미지 링크 수집\n",
    "imgs = driver.find_elements_by_css_selector('img._img')\n",
    "result = []\n",
    "for img in tqdm(imgs):\n",
    "    if 'http' in img.get_attribute('src'):\n",
    "        result.append(img.get_attribute('src'))\n",
    "# print(result)\n",
    "\n",
    "driver.close()\n",
    "print('수집완료')\n",
    "\n",
    "# 파일 저장\n",
    "import os\n",
    "if not os.path.isdir('./{}'.format(key)):\n",
    "    print('폴더생성')\n",
    "    os.mkdir('./{}'.format(key))\n",
    "\n",
    "# 다운로드\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "print(\"다운로드중\")\n",
    "for index, link in tqdm(enumerate(result)):\n",
    "    start = link.rfind('.')\n",
    "    end = link.rfind('&')\n",
    "    # print(link[start:end])\n",
    "    filetype = link[start:end]\n",
    "    if filetype == '.jpg':  # jpg 만 다운\n",
    "        urlretrieve(link, './{}/{}{}{}'.format(key, key, index, filetype))\n",
    "        time.sleep(1)\n",
    "print(\"다운로드 완료\")\n",
    "\n",
    "# import zipfile\n",
    "# zip_file = zipfile.Zipfile('./{}.zip'.format(keyword),'w')\n",
    "# # 이 폴더안에 파일 가져오는거\n",
    "# for image in tqdm(os.listdir('./{}'.format(keyword))):\n",
    "#     zip_file.write('./{}/{}'.format(keyword,image), compress_type=zip_file.ZIP_DEFLATED)\n",
    "# zip_file.close()\n",
    "# print(\"압축완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 전처리\n",
    "#https://keraskorea.github.io/posts/2018-10-24-little_data_powerful_model/\n",
    "\n",
    "- 데이터 전처리와 Augmentation\n",
    "    - 모델이 적은 이미지에서 최대한 많은 정보를 뽑아내서 학습할 수 있도록 우선 이미지를 augment 시켜보도록 하겠습니다. 이미지를 사용할 때마다 임의로 변형을 가함으로써 마치 훨씬 더 많은 이미지를 보고 공부하는 것과 같은 학습 효과를 내는 겁니다. 이를 통해 과적합 (overfitting), 즉 모델이 학습 데이터에만 맞춰지는 것을 방지하고, 새로운 이미지도 잘 분류할 수 있게 합니다.\n",
    "    \n",
    "    \n",
    "# 이미지 데이터를 숫자형 데이터로 변환하기 위해 필요한 파이썬 라이브러리는 다음과 같습니다.\n",
    "\n",
    "os\n",
    "cv2\n",
    "numpy\n",
    "sklearn\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   [ImageDataGenerator]\n",
    "    \n",
    "    \n",
    "    - 이런 전처리 과정을 돕기 위해 케라스는 ImageDataGenerator 클래스를 제공합니다. ImageDataGenerator는 이런 일을 할 수 있죠:\n",
    "\n",
    "    - 학습 도중에 이미지에 임의 변형 및 정규화 적용\n",
    "    - 변형된 이미지를 배치 단위로 불러올 수 있는 generator 생성.\n",
    "    - generator를 생성할 때 flow(data, labels), flow_from_directory(directory) 두 가지 함수를 사용합니다.\n",
    "    - fit_generator, evaluate_generator 함수를 이용하여 generator로 이미지를 불러와서 모델을 학습시킬 수 있습니다.\n",
    "   \n",
    "- rotation_range: 이미지 회전 범위 (degrees)\n",
    "- width_shift, height_shift: 그림을 수평 또는 수직으로 랜덤하게 평행 이동시키는 범위 (원본 가로, 세로 길이에 대한 비율 값)\n",
    "- rescale: 원본 영상은 0-255의 RGB 계수로 구성되는데, 이 같은 입력값은 모델을 효과적으로 학습시키기에 너무 높습니다 (통상적인 learning rate를 사용할 경우). 그래서 이를 1/255로 스케일링하여 0-1 범위로 변환시켜줍니다. 이는 다른 전처리 과정에 앞서 가장 먼저 적용됩니다.\n",
    "- shear_range: 임의 전단 변환 (shearing transformation) 범위\n",
    "- zoom_range: 임의 확대/축소 범위\n",
    "- horizontal_flip: True로 설정할 경우, 50% 확률로 이미지를 수평으로 뒤집습니다. 원본 이미지에 수평 비대칭성이 없을 때 효과적입니다. 즉, 뒤집어도 자연스러울 때 사용하면 좋습니다.\n",
    "- fill_mode 이미지를 회전, 이동하거나 축소할 때 생기는 공간을 채우는 방식\n",
    "케라스 공식 문서를 보시면 이외에도 다양한 인자가 있습니다.\n",
    "\n",
    "\n",
    "   [ImageDataGenerator 디버깅]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#레이어 개수, 그리고 레이어당 필터 개수가 적은 소규모 CNN을 학습시킬 겁니다. \n",
    "#학습 과정에서는 데이터 augmentation 및 드롭아웃 (dropout) 기법을 사용합니다. \n",
    "#드롭아웃은 과적합을 방지할 수 있는 또 다른 방법입니다. \n",
    "#하나의 레이어가 이전 레이어로부터 같은 입력을 두번 이상 받지 못하도록 하여 \n",
    "#데이터 augmentation과 비슷한 효과를 냅니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://rfriend.tistory.com/431"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense,Dropout,Flatten,MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np , pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass_dir = './recycle/glass'\n",
    "can_dir= './recycle/can'\n",
    "plastic_dir= './recycle/plastic'\n",
    "styrofoam_dir='./recycle/styrofoam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "len(os.listdir(glass_r))\n",
    "len(os.listdir(can_r))\n",
    "len(os.listdir(plastic_r))\n",
    "len(os.listdir(styrofoam_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir(glass_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# glass 이름 지정및 사이즈 변경\n",
    "import cv2\n",
    "glass = cv2.imread(\"recycle/glass/gl_coke0.jpg\")\n",
    "print(glass.shape)\n",
    "\n",
    "title= list(os.listdir(\"recycle/glass/\"))  \n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glass 이름 지정및 사이즈 변경\n",
    "for i, re in enumerate(title):\n",
    "    glass = cv2.imread(\"recycle/glass/{}\".format(re))\n",
    "    resize_glass = cv2.resize(glass, (64, 64))\n",
    "    print(\"resize_glass.shape = {0}\".format(resize_glass.shape))   \n",
    "    cv2.imwrite('./recycle/edit/glass_/glass.{}.jpg'.format(i),resize_glass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can  이름 지정 및 사이즈\n",
    "\n",
    "title_can= list(os.listdir(\"recycle/can/\"))  \n",
    "title_can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, re in enumerate(title_can):\n",
    "    can = cv2.imread(\"recycle/can/{}\".format(re))\n",
    "    resize_can = cv2.resize(can, (64, 64))\n",
    "    print(\"resize_can.shape = {0}\".format(resize_can.shape))   \n",
    "    cv2.imwrite('./recycle/edit/can_/can.{}.jpg'.format(i),resize_can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plastic  이름 지정 및 사이즈\n",
    "\n",
    "title_plastic = list(os.listdir(\"recycle/plastic/\"))  \n",
    "\n",
    "# plastic\n",
    "\n",
    "for i, re in enumerate(title_plastic):\n",
    "    plastic = cv2.imread(\"recycle/plastic/{}\".format(re))\n",
    "    resize_plastic = cv2.resize(plastic, (64, 64))\n",
    "    print(\"resize_plastic.shape = {0}\".format(resize_plastic.shape))   \n",
    "    cv2.imwrite('./recycle/edit/plastic_/plastic.{}.jpg'.format(i),resize_plastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# styrofoam  이름 지정 및 사이즈\n",
    "\n",
    "title_styrofoam = list(os.listdir(\"recycle/styrofoam/\"))  \n",
    "\n",
    "# styrofoam\n",
    "\n",
    "for i, re in enumerate(title_styrofoam):\n",
    "    styrofoam = cv2.imread(\"recycle/styrofoam/{}\".format(re))\n",
    "    resize_styrofoam = cv2.resize(styrofoam, (64, 64))\n",
    "    print(\"resize_styrofoam.shape = {0}\".format(resize_styrofoam.shape))   \n",
    "    cv2.imwrite('./recycle/edit/styrofoam_/styrofoam.{}.jpg'.format(i),resize_styrofoam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resize_glass.shape)\n",
    "print(resize_can.shape)\n",
    "print(resize_plastic.shape)\n",
    "print(resize_styrofoam.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규화\n",
    "glass=resize_glass /255\n",
    "can = resize_can /255\n",
    "plastic = resize_plastic/255\n",
    "styrofoam = resize_styrofoam/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABELING\n",
    "\n",
    "http://www.birc.co.kr/2018/02/26/%EC%8B%A4%EC%A0%9C-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-cnn-%EB%AA%A8%EB%8D%B8-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##enumerate\n",
    "\n",
    "반복문 사용 시 몇 번째 반복문인지 확인이 필요할 수 있습니다. 이때 사용합니다.\n",
    "인덱스 번호와 컬렉션의 원소를 tuple형태로 반환합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = [1, 5, 7, 33, 39, 52]\n",
    "for p in enumerate(t):\n",
    "    print(p)\n",
    "\n",
    "#(0, 1)\n",
    "#(1, 5)\n",
    "#(2, 7)\n",
    "#(3, 33)\n",
    "#(4, 39)\n",
    "#(5, 52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실패"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "image_dir ='./recycle/edited/'\n",
    "class_names =['glass','can','plastic','styrofoam']\n",
    "n_class = len(class_names)\n",
    "\n",
    "    \n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_w * image_h * 3\n",
    "\n",
    "X=[]\n",
    "y=[]\n",
    "\n",
    "for idx, sort in enumerate(class_names):\n",
    "    \n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(n_class)]\n",
    "    label[idx] = 1\n",
    "    image_dir = image_dir +\"/\"+ sort\n",
    "    files = glob.glob(image_dir+\"/*.jpg\")\n",
    "    print(sort,'파일길이 :',len(files))\n",
    "    for i,f in enumerate(files):\n",
    "        img = Image.open(f)\n",
    "        img = img.convert('RGB')\n",
    "        data = np.asarray(img)\n",
    "        \n",
    "        X.append(data)\n",
    "        y.append(label)\n",
    "        \n",
    "        if i % 100==0:\n",
    "            print(sort,\":\",f)\n",
    "            \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data, label _ 보류\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.birc.co.kr/2018/02/26/%EC%8B%A4%EC%A0%9C-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-cnn-%EB%AA%A8%EB%8D%B8-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0/\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "TRAIN_DIR = './recycle/edit/'\n",
    "train_folder_list = array(os.listdir(TRAIN_DIR))\n",
    "print(train_folder_list)\n",
    "train_input = []\n",
    "train_label=[]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "#문자열로 구성된 train_folder_list를 숫자형 리스트로 변환합니다\n",
    "integer_encoded = label_encoder.fit_transform(train_folder_list)\n",
    "#앞서 from sklearn.preprocessing import OneHotEncoder를 통해 불러온 \n",
    "#OneHotEncoder 함수를 onehot_encoder라는 변수로 호출합니다.\n",
    "one_hot_encoder = OneHotEncoder(sparse = False)\n",
    "#OneHotEncoder를 사용하기 위해 integer_encoded의 shape을 (10,)에서 (10,1)로 변환합니다. \n",
    "#integer_encoded를 출력하면 다음과 같습니다.\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded),1)\n",
    "#OneHotEncoder를 사용하여 integer_encode를 다음과 같이 변환하여\n",
    "#onehot_encoded 변수에 저장합니다.\n",
    "onehot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "for index in range(len(train_folder_list)):\n",
    "    # path라는 변수에 TRAIN_DIR(D:/python3_project/MNIST_CNN/MNIST/trainingSet/)과\n",
    "    #train_folder_list의 n번째 원소를 합쳐서 path에 저장합니다.\n",
    "    # path 출력 예시는 다음과 같습니다.\n",
    "    path = os.path.join(TRAIN_DIR,train_folder_list[index])\n",
    "    print(path)\n",
    "    #‘/’ : path라는 변수에 ‘/’를 추가합니다. path 출력 예시는 다음과 같습니다.\n",
    "    path = path+'/'\n",
    "    #img_list 변수에 폴더 내 파일명을 저장합니다. \n",
    "    #img_list 출력 예시는 다음과 같습니다.\n",
    "    img_list = os.listdir(path)\n",
    "    print(img_list)\n",
    "    #img_path = os.path.join(path, img) : img_path에 정확한 이미지 경로를 저장합니다. \n",
    "    \n",
    "    for img in img_list:\n",
    "        img_path = os.path.join(path,img)\n",
    "        print(img_path)\n",
    "        img = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE) # 흑백으로 불러오기 \n",
    "        train_input.append([np.array(img)])\n",
    "        train_label.append([np.array(onehot_encoded[index])])\n",
    "        \n",
    "train_input = np.reshape(train_input,(-1,4096))\n",
    "train_label = np.reshape(train_label,(-1,4))\n",
    "train_input = np.array(train_input).astype(np.float32)\n",
    "train_label = np.array(train_label).astype(np.float32)\n",
    "\n",
    "np.save('train_data.npy',train_input)\n",
    "np.save('train_label.npy',train_label)\n",
    "\n",
    "\n",
    "data_set =[]\n",
    "data_set.append([train_input,train_label])\n",
    "\n",
    "print(data_set)\n",
    "\n",
    "print(train_input.shape)\n",
    "\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prubt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = data_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECYCLING IMAGE CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os,glob,numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout,Activation,Dense\n",
    "from tensorflow.keras.layers import Flatten,Conv2D,MaxPooling2D\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image_dir ='C:/Users/ICT01_14/Documents/seoyoon/recycle/edit'\n",
    "class_names =['glass','can','plastic','styrofoam']\n",
    "n_class = len(class_names)\n",
    "\n",
    "    \n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_w * image_h * 3\n",
    "\n",
    "X=[]\n",
    "y=[]\n",
    "\n",
    "for idx, sort in enumerate(class_names):\n",
    "    \n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(n_class)]\n",
    "    label[idx] = 1\n",
    "    \n",
    "    img_dir = image_dir +\"/\"+ sort\n",
    "    files = glob.glob(img_dir+\"/*.jpg\")\n",
    "    print(sort,'파일길이 :',len(files))\n",
    "    \n",
    "    for i,f in enumerate(files):\n",
    "        \n",
    "        img = Image.open(f)\n",
    "        img = img.convert('RGB')\n",
    "        img = img.resize((image_w,image_h))\n",
    "        data = np.asarray(img)\n",
    "        \n",
    "        X.append(data)\n",
    "        y.append(label)\n",
    "        \n",
    "#         if i % 100==0:\n",
    "#             print(sort,\":\",f)\n",
    "            \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "               \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "np.save(\"./recycle/model/numpy_data/multi_image_data.npy\", xy)\n",
    "\n",
    "print(\"ok\", len(y))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load(\"./recycle/model/numpy_data/multi_image_data.npy\",allow_pickle =True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = ['glass','can','plastic','styrofoam']\n",
    "n_class = len(class_name)\n",
    "\n",
    "X_train = X_train.astype(float)/255\n",
    "X_test = X_test.astype(float)/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ICT01_14\\.conda\\envs\\tf_test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#with K.tf_ops.device('/device:GPU:0'):\n",
    "model =Sequential()\n",
    "model.add(Conv2D(32,(3,3),padding='same',input_shape = X_train.shape[1:],\n",
    "                activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(n_class,activation ='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer='adam',\n",
    "              metrics = ['accuracy'])\n",
    "model_dir ='./recycle/model'\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "model_path = model_dir + '/multi_img_classification.model'\n",
    "checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss',verbose = 1,save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss',patience=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/lsjsj92/keras_basic/blob/master/7.%20predict_multi_img_with_CNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y_train,batch_size = 32,epochs=30,validation_data=(X_test,y_test),\n",
    "                   callbacks=[checkpoint,early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, glob,numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "image_directory ='C:/Users/ICT01_14/Documents/seoyoon/recycle/model'\n",
    "class_names =['glass','can','plastic','styrofoam']\n",
    "\n",
    "prediction = model.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pixels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image generator\n",
    "\n",
    "- https://tykimos.github.io/2017/06/10/CNN_Data_Augmentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout,Activation,Dense\n",
    "from tensorflow.keras.layers import Flatten,Conv2D,MaxPooling2D\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "import cv2\n",
    "\n",
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1차 . 정확도 48%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glass 파일길이 : 1266\n",
      "can 파일길이 : 507\n",
      "plastic 파일길이 : 451\n",
      "styrofoam 파일길이 : 774\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "image_dir ='C:/Users/ICT01_14/Documents/seoyoon/recycle/edit'\n",
    "class_names =['glass','can','plastic','styrofoam']\n",
    "n_class = len(class_names)\n",
    "\n",
    "    \n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_w * image_h * 3\n",
    "\n",
    "X=[]\n",
    "y=[]\n",
    "\n",
    "for idx, sort in enumerate(class_names):\n",
    "    \n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(n_class)]\n",
    "    label[idx] = 1\n",
    "    \n",
    "    img_dir = image_dir +\"/\"+ sort\n",
    "    files = glob.glob(img_dir+\"/*.jpg\")\n",
    "    print(sort,'파일길이 :',len(files))\n",
    "    \n",
    "    for i,f in enumerate(files):\n",
    "        \n",
    "        img = Image.open(f)\n",
    "        img = img.convert('RGB')\n",
    "        img = img.resize((image_w,image_h))\n",
    "        data = np.asarray(img)\n",
    "        \n",
    "        X.append(data)\n",
    "        y.append(label)\n",
    "        \n",
    "#         if i % 100==0:\n",
    "#             print(sort,\":\",f)\n",
    "            \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok 2998\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "np.save(\"./recycle/model/numpy_data/multi_image_data.npy\", xy)\n",
    "\n",
    "print(\"ok\", len(y))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load(\"./recycle/model/numpy_data/multi_image_data.npy\",allow_pickle =True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with K.tf_ops.device('/device:GPU:0'):\n",
    "model =Sequential()\n",
    "model.add(Conv2D(32,(3,3),padding='same',input_shape = X_train.shape[1:],\n",
    "                activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(n_class,activation ='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer='adam',\n",
    "              metrics = ['accuracy'])\n",
    "model_dir ='./recycle/model'\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "model_path = model_dir + '/multi_img_classification.model'\n",
    "checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss',verbose = 1,save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss',patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale =1./255,shear_range =0.2,rotation_range = 20,\n",
    "                              width_shift_range =0.05,height_shift_range=0.05,zoom_range = 0.2,\n",
    "                              horizontal_flip=True,vertical_flip =True,fill_mode = 'nearest')\n",
    "\n",
    "data_flow = datagen.flow(X_train,y_train,batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.3528 - accuracy: 0.8597\n",
      "Epoch 2/70\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.3199 - accuracy: 0.8739\n",
      "Epoch 3/70\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 0.3330 - accuracy: 0.8681\n",
      "Epoch 4/70\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.3302 - accuracy: 0.8701\n",
      "Epoch 5/70\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.3172 - accuracy: 0.8732\n",
      "Epoch 6/70\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3183 - accuracy: 0.8723\n",
      "Epoch 7/70\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 0.3217 - accuracy: 0.8721\n",
      "Epoch 8/70\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 0.2955 - accuracy: 0.8832\n",
      "Epoch 9/70\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 0.3029 - accuracy: 0.8755\n",
      "Epoch 10/70\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.3117 - accuracy: 0.8715\n",
      "Epoch 11/70\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.2898 - accuracy: 0.8796\n",
      "Epoch 12/70\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.2976 - accuracy: 0.8799\n",
      "Epoch 13/70\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.2914 - accuracy: 0.8854\n",
      "Epoch 14/70\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 0.2932 - accuracy: 0.8830\n",
      "Epoch 15/70\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.2928 - accuracy: 0.8823\n",
      "Epoch 16/70\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 0.2793 - accuracy: 0.8864\n",
      "Epoch 17/70\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 0.2966 - accuracy: 0.8869\n",
      "Epoch 18/70\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.2923 - accuracy: 0.8833\n",
      "Epoch 19/70\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 0.2882 - accuracy: 0.8887\n",
      "Epoch 20/70\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 0.2862 - accuracy: 0.8823\n",
      "Epoch 21/70\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 0.2724 - accuracy: 0.8925\n",
      "Epoch 22/70\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 0.2864 - accuracy: 0.8856\n",
      "Epoch 23/70\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.2845 - accuracy: 0.8859\n",
      "Epoch 24/70\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.2811 - accuracy: 0.8832\n",
      "Epoch 25/70\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 0.2620 - accuracy: 0.8935\n",
      "Epoch 26/70\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 0.2723 - accuracy: 0.8930\n",
      "Epoch 27/70\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.2642 - accuracy: 0.8917\n",
      "Epoch 28/70\n",
      "100/100 [==============================] - 26s 265ms/step - loss: 0.2846 - accuracy: 0.8897\n",
      "Epoch 29/70\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.2710 - accuracy: 0.8924\n",
      "Epoch 30/70\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 0.2525 - accuracy: 0.9017\n",
      "Epoch 31/70\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.2545 - accuracy: 0.9004\n",
      "Epoch 32/70\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.2590 - accuracy: 0.8918\n",
      "Epoch 33/70\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.2454 - accuracy: 0.8990\n",
      "Epoch 34/70\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 0.2443 - accuracy: 0.9025\n",
      "Epoch 35/70\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 0.2632 - accuracy: 0.8965\n",
      "Epoch 36/70\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.2394 - accuracy: 0.9042\n",
      "Epoch 37/70\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 0.2609 - accuracy: 0.8964\n",
      "Epoch 38/70\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 0.2597 - accuracy: 0.8982\n",
      "Epoch 39/70\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.2466 - accuracy: 0.9017\n",
      "Epoch 40/70\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.2467 - accuracy: 0.9057\n",
      "Epoch 41/70\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.2463 - accuracy: 0.9009\n",
      "Epoch 42/70\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.2540 - accuracy: 0.9005\n",
      "Epoch 43/70\n",
      "100/100 [==============================] - 24s 245ms/step - loss: 0.2513 - accuracy: 0.8952\n",
      "Epoch 44/70\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.2411 - accuracy: 0.9049\n",
      "Epoch 45/70\n",
      "100/100 [==============================] - 27s 271ms/step - loss: 0.2222 - accuracy: 0.9086\n",
      "Epoch 46/70\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.2536 - accuracy: 0.9023\n",
      "Epoch 47/70\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 0.2404 - accuracy: 0.9024\n",
      "Epoch 48/70\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 0.2266 - accuracy: 0.9126\n",
      "Epoch 49/70\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 0.2204 - accuracy: 0.9107\n",
      "Epoch 50/70\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 0.2131 - accuracy: 0.9139\n",
      "Epoch 51/70\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 0.2350 - accuracy: 0.9069\n",
      "Epoch 52/70\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 0.2282 - accuracy: 0.9099\n",
      "Epoch 53/70\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 0.2361 - accuracy: 0.9055\n",
      "Epoch 54/70\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 0.2371 - accuracy: 0.9095\n",
      "Epoch 55/70\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.2251 - accuracy: 0.9142\n",
      "Epoch 56/70\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 0.2215 - accuracy: 0.9124\n",
      "Epoch 57/70\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 0.2059 - accuracy: 0.9169\n",
      "Epoch 58/70\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 0.2176 - accuracy: 0.9137\n",
      "Epoch 59/70\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 0.2035 - accuracy: 0.9175\n",
      "Epoch 60/70\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.2171 - accuracy: 0.9143\n",
      "Epoch 61/70\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.2098 - accuracy: 0.9162\n",
      "Epoch 62/70\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.2126 - accuracy: 0.9149\n",
      "Epoch 63/70\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.2294 - accuracy: 0.9130\n",
      "Epoch 64/70\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.2028 - accuracy: 0.9191\n",
      "Epoch 65/70\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.2068 - accuracy: 0.9213\n",
      "Epoch 66/70\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.2194 - accuracy: 0.9162\n",
      "Epoch 67/70\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.2150 - accuracy: 0.9180\n",
      "Epoch 68/70\n",
      "100/100 [==============================] - 25s 245ms/step - loss: 0.2049 - accuracy: 0.9203\n",
      "Epoch 69/70\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 0.2090 - accuracy: 0.9169\n",
      "Epoch 70/70\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 0.1981 - accuracy: 0.9230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x232363f8dc8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size =100\n",
    "steps_per_epoch =100\n",
    "model.fit_generator(data_flow,epochs = 70 , steps_per_epoch = steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 1s 783us/step\n",
      "정확도 : 0.4893\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2차 정확도 : \n",
    "- drop 0.25 => 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with K.tf_ops.device('/device:GPU:0'):\n",
    "model =Sequential()\n",
    "model.add(Conv2D(32,(3,3),padding='same',input_shape = X_train.shape[1:],\n",
    "                activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(n_class,activation ='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer='adam',\n",
    "              metrics = ['accuracy'])\n",
    "model_dir ='./recycle/model'\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "model_path = model_dir + '/multi_img_classification.model'\n",
    "checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss',verbose = 1,save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss',patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale =1./255,shear_range =0.2,rotation_range = 20,\n",
    "                              width_shift_range =0.05,height_shift_range=0.05,zoom_range = 0.2,\n",
    "                              horizontal_flip=True,vertical_flip =True,fill_mode = 'nearest')\n",
    "\n",
    "data_flow = datagen.flow(X_train,y_train,batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 1.2584 - accuracy: 0.4705\n",
      "Epoch 2/70\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.9096 - accuracy: 0.6270\n",
      "Epoch 3/70\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 0.8497 - accuracy: 0.6588\n",
      "Epoch 4/70\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.8138 - accuracy: 0.6756\n",
      "Epoch 5/70\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.7800 - accuracy: 0.6907\n",
      "Epoch 6/70\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.7429 - accuracy: 0.7050\n",
      "Epoch 7/70\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.7216 - accuracy: 0.7174\n",
      "Epoch 8/70\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 0.7065 - accuracy: 0.7183\n",
      "Epoch 9/70\n",
      "100/100 [==============================] - 26s 265ms/step - loss: 0.6861 - accuracy: 0.7282\n",
      "Epoch 10/70\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.6658 - accuracy: 0.7339\n",
      "Epoch 11/70\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.6448 - accuracy: 0.7408\n",
      "Epoch 12/70\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 0.6291 - accuracy: 0.7458\n",
      "Epoch 13/70\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 0.6093 - accuracy: 0.7531\n",
      "Epoch 14/70\n",
      "100/100 [==============================] - 27s 272ms/step - loss: 0.6011 - accuracy: 0.7605\n",
      "Epoch 15/70\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 0.5837 - accuracy: 0.7614\n",
      "Epoch 16/70\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.5918 - accuracy: 0.7618\n",
      "Epoch 17/70\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 0.5624 - accuracy: 0.7727\n",
      "Epoch 18/70\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.5610 - accuracy: 0.7704\n",
      "Epoch 19/70\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.5422 - accuracy: 0.7856\n",
      "Epoch 20/70\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 0.5293 - accuracy: 0.7872\n",
      "Epoch 21/70\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 0.5360 - accuracy: 0.7850\n",
      "Epoch 22/70\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 0.5046 - accuracy: 0.7989\n",
      "Epoch 23/70\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 0.4970 - accuracy: 0.8021\n",
      "Epoch 24/70\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4776 - accuracy: 0.8081\n",
      "Epoch 25/70\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4812 - accuracy: 0.8092\n",
      "Epoch 26/70\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.4754 - accuracy: 0.8050\n",
      "Epoch 27/70\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.4669 - accuracy: 0.8169\n",
      "Epoch 28/70\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.4645 - accuracy: 0.8185\n",
      "Epoch 29/70\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 0.4529 - accuracy: 0.8139\n",
      "Epoch 30/70\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.4336 - accuracy: 0.8235\n",
      "Epoch 31/70\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.4427 - accuracy: 0.8269\n",
      "Epoch 32/70\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 0.4441 - accuracy: 0.8240\n",
      "Epoch 33/70\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.4249 - accuracy: 0.8314\n",
      "Epoch 34/70\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.4193 - accuracy: 0.8320\n",
      "Epoch 35/70\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 0.4087 - accuracy: 0.8369\n",
      "Epoch 36/70\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 0.4111 - accuracy: 0.8373\n",
      "Epoch 37/70\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.4071 - accuracy: 0.8391\n",
      "Epoch 38/70\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 0.3881 - accuracy: 0.8452\n",
      "Epoch 39/70\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 0.4047 - accuracy: 0.8428\n",
      "Epoch 40/70\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 0.3831 - accuracy: 0.8485\n",
      "Epoch 41/70\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3690 - accuracy: 0.8517\n",
      "Epoch 42/70\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.3810 - accuracy: 0.8481\n",
      "Epoch 43/70\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.3688 - accuracy: 0.8544\n",
      "Epoch 44/70\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 0.3713 - accuracy: 0.8529\n",
      "Epoch 45/70\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.3672 - accuracy: 0.8591\n",
      "Epoch 46/70\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.3530 - accuracy: 0.8595\n",
      "Epoch 47/70\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 0.3558 - accuracy: 0.8611\n",
      "Epoch 48/70\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.3563 - accuracy: 0.8596\n",
      "Epoch 49/70\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3396 - accuracy: 0.8654\n",
      "Epoch 50/70\n",
      "100/100 [==============================] - 26s 265ms/step - loss: 0.3424 - accuracy: 0.8694\n",
      "Epoch 51/70\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 0.3381 - accuracy: 0.8621\n",
      "Epoch 52/70\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 0.3440 - accuracy: 0.8623\n",
      "Epoch 53/70\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.3271 - accuracy: 0.8728\n",
      "Epoch 54/70\n",
      "100/100 [==============================] - 24s 245ms/step - loss: 0.3231 - accuracy: 0.8751\n",
      "Epoch 55/70\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.3360 - accuracy: 0.8680\n",
      "Epoch 56/70\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3224 - accuracy: 0.8737\n",
      "Epoch 57/70\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.3261 - accuracy: 0.8698\n",
      "Epoch 58/70\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.3094 - accuracy: 0.8789\n",
      "Epoch 59/70\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.3178 - accuracy: 0.8764\n",
      "Epoch 60/70\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 0.3186 - accuracy: 0.8796\n",
      "Epoch 61/70\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 0.3013 - accuracy: 0.8864\n",
      "Epoch 62/70\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 0.2890 - accuracy: 0.8857\n",
      "Epoch 63/70\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.2861 - accuracy: 0.8894\n",
      "Epoch 64/70\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.3008 - accuracy: 0.8802\n",
      "Epoch 65/70\n",
      "100/100 [==============================] - 25s 245ms/step - loss: 0.2870 - accuracy: 0.8893\n",
      "Epoch 66/70\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.2937 - accuracy: 0.8864\n",
      "Epoch 67/70\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 0.2807 - accuracy: 0.8900\n",
      "Epoch 68/70\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.2807 - accuracy: 0.8909\n",
      "Epoch 69/70\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.2756 - accuracy: 0.8922\n",
      "Epoch 70/70\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.2937 - accuracy: 0.8854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x232002c3e88>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size =100\n",
    "steps_per_epoch =100\n",
    "model.fit_generator(data_flow,epochs = 70 , steps_per_epoch = steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "734px",
    "left": "21px",
    "top": "0px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
