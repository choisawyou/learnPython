{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import os, glob, numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy import array\n",
    "from PIL import Image as img\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/ICT01_09/Documents/CNN'\n",
    "categories = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories   # 폴더명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(categories)\n",
    "nb_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = cv2.imread(\"st/st0.jpg\")\n",
    "#print(img.shape)  # 확인 \n",
    "#title = list(os.listdir(\"st/\"))\n",
    "\n",
    "#for i, re in enumerate(title):             # 번호 생성 \n",
    "#    img = cv2.imread(\"st/{}\".format(re))  # st/파일명 을 말한다 \n",
    "#    resize_img = cv2.resize(img, (64, 64))\n",
    "#    print(\"resize_img.shape = {0}\".format(resize_img.shape))   \n",
    "#    cv2.imwrite('styrofoam/st.{}.jpg'.format(i),resize_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for index in range(len(categories)):\n",
    "#    path = os.path.join(data_dir, categories[index])\n",
    "#    path = path + '/'\n",
    "#    print(path)\n",
    "#    img_list = os.listdir(path)\n",
    "#    for img in img_list:\n",
    "#        img_path = os.path.join(path, img)\n",
    "#        img = cv2.imread(img_path)\n",
    "#        img = cv2.resize(img,(64,64))\n",
    "#        train_input.append([np.array(img)])\n",
    "#        train_label.append([np.array(onehot_encoder[index])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 크기 줄여주기 \n",
    "image_w = 128\n",
    "image_h = 128\n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for idex, categorie in enumerate(categories):\n",
    "    \n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idex] = 1\n",
    "\n",
    "    image_dir = data_dir + \"/\" + categorie + '/'\n",
    "    \n",
    "    for top, dir, f in os.walk(image_dir):\n",
    "        for filename in f:\n",
    "            print(image_dir+filename)\n",
    "            img = cv2.imread(image_dir+filename)\n",
    "            img= cv2.resize(img,None,fx=image_w/img.shape[0], fy=image_h/img.shape[1])\n",
    "            X.append(img/256)\n",
    "            Y.append(label)\n",
    "            \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "xy = (X_train, X_test, Y_train, Y_test)\n",
    "np.save(\"./img_data.npy\", xy)\n",
    "\n",
    "print(\"ok\", len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 전처리 ( 수작업)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 이미지 -> 숫자형 데이터로 변환 \n",
    "# 필요한 라이브러리 : os, cv2, numpy, sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   # >> 디렉토리 경로 호출\n",
    "import cv2   # >> 이미지 파일 호출\n",
    "import numpy as np   # >> 데이터 처리에 사용\n",
    "from numpy import array  # >> 리스트를 array 형태로 만든다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder  #>> 문자로된 폴더리스트를 숫자형 array로 \n",
    "from sklearn.preprocessing import OneHotEncoder #>> 숫자형 array 를 one-hot-encode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/ICT01_09/Documents/train/'  # 이미지 데이터셋의 경로\n",
    "folder_list = array(os.listdir(data_dir))  # 내부 폴더명을 array 형태로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(folder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []  # X값 \n",
    "Y = []  # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(folder_list)\n",
    "# 문자열로 구성된 folder_list 를 숫자형 리스트로 변환\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# 위에서 부른 OneHotEncoder 함수를 onehot_encoder 라는 변수로 호출\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded),1)\n",
    "##>> (4.) 에서 (4,1) 로 변한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)  # integer_encoded 를 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_w= 128\n",
    "image_h = 128\n",
    "\n",
    "for index in range(len(folder_list)): # 0~3까지 4번 반복된다\n",
    "    path = os.path.join(data_dir, folder_list[index] )\n",
    "    path = path+'/'\n",
    "    img_list = os.listdir(path)  # 폴더내 파일명을 저장한다(모든파일명 호출) \n",
    "    for img in img_list:\n",
    "        img_path = os.path.join(path,img)  # img_path : 정확한 이미지 경로\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기서부터 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 전처리 ( 수작업)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 이미지 이름 바꾸기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#공부하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 이미지 부풀리기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tykimos.github.io/2017/06/10/CNN_Data_Augmentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더별 원래 데이터 갯수를 확인해보자  \n",
    "import os, glob, numpy as np\n",
    "from numpy import array\n",
    "data_dir = \"C:/Users/ICT01_09/Documents/train/\" # 폴더\n",
    "categories = array(os.listdir(data_dir))\n",
    "\n",
    "for idx, trash in enumerate(categories):  \n",
    "    image_dir = data_dir + '/' + trash   # 4개 폴더 주소 출력 \n",
    "    files = glob.glob(image_dir+\"/*.jpg\")\n",
    "    print(trash, \" 파일 길이 : \", len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터를 부풀리자!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 랜덤시드 고정시키기\n",
    "np.random.seed(5)\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=15,  # 원본이미지 회전\n",
    "                                   width_shift_range=0.1, # 수평방향이동\n",
    "                                   height_shift_range=0.1, # 수직방향 이동\n",
    "                                   shear_range=0.5,           #시계반대방향변형\n",
    "                                   zoom_range=[0.8, 2.0],  # 확대, 축소\n",
    "                                   horizontal_flip=True, # 수평방향 뒤집기\n",
    "                                   vertical_flip=True, # 수직방향 뒤집기 \n",
    "                                   fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#먼저 can 의 이미지 늘리기 (507-> 10000)\n",
    "for idx, trash in enumerate(categories):  \n",
    "    image_dir = data_dir + '/' + trash    # 4개 폴더 주소 출력 \n",
    "    files = glob.glob(image_dir+\"/*.jpg\") # 모든 사진 이름 출력\n",
    "    if trash == 'can':                     # can 폴더 사진만 가져온다 \n",
    "        for i in range(len(files)):\n",
    "            img = load_img(files[i])  # 모든 can 의 이미지를 불러왔다 \n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,) + x.shape)\n",
    "            i=0\n",
    "            for batch in train_datagen.flow(x,batch_size=1, save_to_dir = 'C:/Users/ICT01_09/Documents/train/new_can'\n",
    "                                            ,save_prefix = 'tri', save_format='jpg'):\n",
    "                i += 1\n",
    "                if i > 250:  # 200 이면 9999\n",
    "                    break\n",
    "\n",
    "\n",
    "image_dir = data_dir + '/' + 'new_can'   # 4개 폴더 주소 출력 \n",
    "files = glob.glob(image_dir+\"/*.jpg\")\n",
    "print(\" 파일 길이 : \", len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#먼저 glass의 이미지 늘리기 (1266-> 10000)\n",
    "for idx, trash in enumerate(categories):  \n",
    "    image_dir = data_dir + '/' + trash    # 4개 폴더 주소 출력 \n",
    "    files = glob.glob(image_dir+\"/*.jpg\") # 모든 사진 이름 출력\n",
    "    if trash == 'glass':                     # glass 폴더 사진만 가져온다 \n",
    "        for i in range(len(files)):\n",
    "            img = load_img(files[i])  # 모든 glass 의 이미지를 불러왔다 \n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,) + x.shape)\n",
    "            i=0\n",
    "            for batch in train_datagen.flow(x,batch_size=1, save_to_dir = 'C:/Users/ICT01_09/Documents/train/new_glass'\n",
    "                                            ,save_prefix = 'tri', save_format='jpg'):\n",
    "                i += 1\n",
    "                if i > 100:\n",
    "                    break\n",
    "\n",
    "\n",
    "image_dir = data_dir + '/' + 'new_glass'   # 4개 폴더 주소 출력 \n",
    "files = glob.glob(image_dir+\"/*.jpg\")\n",
    "print(\" 파일 길이 : \", len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#먼저 pet 의 이미지 늘리기 (-> 10000)\n",
    "for idx, trash in enumerate(categories):  \n",
    "    image_dir = data_dir + '/' + trash    # 4개 폴더 주소 출력 \n",
    "    files = glob.glob(image_dir+\"/*.jpg\") # 모든 사진 이름 출력\n",
    "    if trash == 'pet':                     # can 폴더 사진만 가져온다 \n",
    "        for i in range(len(files)):\n",
    "            img = load_img(files[i])  # 모든 can 의 이미지를 불러왔다 \n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,) + x.shape)\n",
    "            i=0\n",
    "            for batch in train_datagen.flow(x,batch_size=1, save_to_dir = 'C:/Users/ICT01_09/Documents/train/new_pet'\n",
    "                                            ,save_prefix = 'tri', save_format='jpg'):\n",
    "                i += 1\n",
    "                if i > 200:\n",
    "                    break\n",
    "\n",
    "\n",
    "image_dir = data_dir + '/' + 'new_pet'   # 4개 폴더 주소 출력 \n",
    "files = glob.glob(image_dir+\"/*.jpg\")\n",
    "print(\" 파일 길이 : \", len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#먼저 styrofoam 의 이미지 늘리기 (451-> 10000)\n",
    "for idx, trash in enumerate(categories):  \n",
    "    image_dir = data_dir + '/' + trash    # 4개 폴더 주소 출력 \n",
    "    files = glob.glob(image_dir+\"/*.jpg\") # 모든 사진 이름 출력\n",
    "    if trash == 'styrofoam':                     # can 폴더 사진만 가져온다 \n",
    "        for i in range(len(files)):\n",
    "            img = load_img(files[i])  # 모든 can 의 이미지를 불러왔다 \n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,) + x.shape)\n",
    "            i=0\n",
    "            for batch in train_datagen.flow(x,batch_size=1, save_to_dir = 'C:/Users/ICT01_09/Documents/train/new_styrofoam'\n",
    "                                            ,save_prefix = 'tri', save_format='jpg'):\n",
    "                i += 1\n",
    "                if i > 200:\n",
    "                    break\n",
    "\n",
    "\n",
    "image_dir = data_dir + '/' + 'new_styrofoam'   # 4개 폴더 주소 출력 \n",
    "files = glob.glob(image_dir+\"/*.jpg\")\n",
    "print(\" 파일 길이 : \", len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  이미지 배열화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new_can', 'new_pet', 'new_glass', 'new_styrofoam']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np   # >> 데이터 처리에 사용\n",
    "from numpy import array\n",
    "\n",
    "data_dir = \"C:/Users/ICT01_09/Documents/train/\" # 폴더\n",
    "\n",
    "#categories = array(os.listdir(data_dir)) # 각 카테고리 폴더에서 불러오기\n",
    "categories = ['new_can', 'new_pet', 'new_glass', 'new_styrofoam']\n",
    "print(categories)\n",
    "nb_classes = len(categories)\n",
    "print(nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_w = 64; image_h = 64  # 이미지 크기 : 64x64 \n",
    "X = []\n",
    "Y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, trash in enumerate(categories):     #>>idx=0,1,2,3 생성/ trash 는 폴더이름 \n",
    "    label = [0 for i in range(nb_classes)]   # label 을 0으로 초기화->[0,0,0,0]\n",
    "    label[idx]=1  \n",
    "      #[1, 0, 0, 0] \n",
    "      #[0, 1, 0, 0]\n",
    "      #[0, 0, 1, 0]\n",
    "      #[0, 0, 0, 1]\n",
    "    image_dir = data_dir + '/' + trash   # 4개 폴더 주소 출력 \n",
    "    files = glob.glob(image_dir+\"/*.jpg\") # 모든 사진 주소 가져온다 \n",
    "    \n",
    "    for i, f in enumerate(files): #그룹별 사진파일마다 0부터 숫자부여(i는 숫자, f는 주소)\n",
    "        img = Image.open(f) # 이미지 하나하나 불러온다고 생각하자 \n",
    "        img = img.convert('RGB')  # 그레이로 하겠다\n",
    "        img = img.resize((image_w, image_h)) #사이즈 다시 설정\n",
    "        data = np.asarray(img)  # img 값을 넘파이 라이브러리 이용해 가지고와서 저장한다\n",
    "        \n",
    "        X.append(data)  # X data\n",
    "        Y.append(label) # Y 는 리스트형으로 저장 \n",
    "        \n",
    "        \n",
    "        #if i % 3500 == 0: # 1500번째 사진을 가져온다 ( 확인용 ) \n",
    "         #   print(trash,\":\",f)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "## Y 는 아래처럼 저장이 된다 \n",
    "## can 이면 [1,0,0,0]\n",
    "## glass이면  [0,1,1,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 이미지 (훈련데이터/ 테스트 데이터 나누기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "40000\n",
      "30000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X,Y)\n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy=(X_train,X_test,y_train,y_test)\n",
    "np.save(\"./multi_image_data.npy\",xy)  ## numpy 라이브러리 이용해서 외부파일로 저장한다\n",
    "                                     # 배열을 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_save_load = np.load('./multi_image_data.npy')\n",
    "#x_save_load  # 배열 부른것 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 메모리 절약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# 메모리 절약하기 \n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.90\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load('./multi_image_data.npy',allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 64, 64, 3)\n",
      "30000\n",
      "(10000, 64, 64, 3)\n",
      "[[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train.shape[0])\n",
    "print(X_test.shape)\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(float)/255\n",
    "X_test = X_test.astype(float)/ 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 모델링\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델1 : 컨볼루션층2개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with K.tf_ops.device('/device:GPU:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())  # flatten 시킨다 \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # 오차함수는 categorical_crossentropy 를 사용하고\n",
    "    # 최적화함수는 adam 을 사용한다\n",
    "    # metrics -> 평가기준을 말한다 (학습과정중 제대로학습하는지) ,일반적으로 accuracy 를 삽입한다 )\n",
    "    # --> 내부적으로 categorical_accuracy 함수를 이용하여 정확도 계산이 가능하다 \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 모델의 성과를 저장하고, 모델의 최적화단계에서 학습을 자동중단하게 설정\n",
    "    #--> 10회 이상 모델의 성과 향상이 없으면 자동으로 학습이 중단된다 \n",
    "    model_dir = 'C:/Users/ICT01_09/Documents/model'\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_path = model_dir + '/multi_img_classification.model'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6)   # 과적합된다는 신호가 오는데 6번정도오면 끊는다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout 을 하는 이유??\n",
    "# >> 신경망을 위해 사용되는 규제 기법 중에서 가장 효과적\n",
    "# >> 드롭아웃을 적용하면 훈련하는 동안 무작위로 층의 일부 출력 특성을 제외시킨다(0으로 만든다)\n",
    "\n",
    "## 과대적합을 방지하는 방법 : \n",
    "#1. 훈련데이터 더 모은다\n",
    "#2. 네트워크의 용량 감소\n",
    "#3. 가중치 규제 추가( 가중치가 작은 값을 가지도록 하는 것이다 -> 아래 모델 2에서 해보기  )\n",
    "#4. 드롭아웃 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<학습 조기종료시키기>\n",
    "# https://tykimos.github.io/2017/07/09/Early_Stopping/\n",
    "# 과적합을 방지하기 위해 조기 종료한다 \n",
    "#>> EarlyStopping 이라는 함수를 사용하여, 더 이상의 개선의 여지가 없을때 학습 종료\n",
    "#>> fit 함수에서 Earlystopping 이라는 콜백함수를 설정하면 된다 \n",
    "\n",
    "#checkpoint 를 확인해보자 \n",
    "#monitor='val_loss' ( 관찰하고자 하는 항목 ) \n",
    "#verbose : 얼마나 자세하게 정보를 표현할 것인가 ( 0,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_split=0.2, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit 함수의 결과\n",
    "# loss(에포크마다 훈련 손실값), acc(에포크마다 훈련 정확도),\n",
    "# val_loss(에포크마다 검증 손실값), val_acc(에포크마다 검증 정확도) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델학습 과정 표시하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()  # x축은 에포크수, y 축은 손실값 \n",
    "acc_ax = loss_ax.twinx()   \n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss') # 훈련 손실값 (노란색)\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss') #검증(val=validation) 손실값( 빨간색)\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['accuracy'], 'b', label='train acc') #훈련 정확도 ( 파란색)\n",
    "acc_ax.plot(history.history['val_accuracy'], 'g', label='val acc')# 검증정확도(녹색) \n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch =50 이였는데, 20에서 멈춤 ( 자기딴에는 과적합이라고 생각해서 )\n",
    "# epochs 값이 지나치게 클 수록 과적합이 발생할 수 있다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실값 작을수록 좁다 . 손실값을 최소화하도록 optimizer 을 쓴다\n",
    "# https://tykimos.github.io/2017/07/09/Training_Monitoring/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 그림을 넣고 예측해보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"C:/Users/ICT01_09/Documents/train/other_test\"\n",
    "image_w = 64\n",
    "image_h = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "filenames = []\n",
    "files = glob.glob(test_dir+\"/*.*\") # 모든 파일이 출력된다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, f in enumerate(files):  \n",
    "    img = Image.open(f)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((image_w, image_h))\n",
    "    data = np.asarray(img)\n",
    "    filenames.append(f)\n",
    "    \n",
    "    X.append(data)\n",
    "\n",
    "X= np.array(X)\n",
    "model = load_model(\"./model/multi_img_classification.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.astype(float)/255\n",
    "#X_test = X_test.astype(float)/ 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)}) #>> 넘파이 출력옵션 변경하는것! (소수점3자리까지)\n",
    "cnt = 0\n",
    "for i in prediction:\n",
    "    pre_ans = i.argmax()  # 예측 레이블  # argmax : 함수를 최대로 만들기 위한 x 값 --> 즉 첫번째에서는 3번째만 1이므로 2가 출력됨\n",
    "    print(i)\n",
    "    print(pre_ans)\n",
    "    pre_ans_str = ''\n",
    "    if pre_ans == 0: pre_ans_str = \"캔\"\n",
    "    elif pre_ans == 1: pre_ans_str = \"플라스틱\"\n",
    "    elif pre_ans == 2: pre_ans_str = \"유리\"\n",
    "    else: pre_ans_str = \"스티로품\"\n",
    "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
    "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델2 : 가중치 규제를 했을때 ( 과적합 막는다 )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with K.tf_ops.device('/device:GPU:0'):\n",
    "    model2 = Sequential()\n",
    "    model2.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
    "    model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model2.add(Dropout(0.25))\n",
    "    \n",
    "    model2.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "    model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model2.add(Dropout(0.25))\n",
    "    \n",
    "    model2.add(Conv2D(128, (3,3), padding=\"same\", activation='relu'))\n",
    "    model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model2.add(Dropout(0.25))\n",
    "    \n",
    "    model2.add(Flatten())  \n",
    "    model2.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  ## 여기서 l2 가중치를 규제한다 (과대적합 막는다)\n",
    "                                                                       #>>> 가중치행렬의 모든 원소를 제곱하고 0.001을곱한다-> 네트워크 전체손실에 더해진다 \n",
    "                                                                       #>> 훈련할때만 추가된다 \n",
    "    model2.add(Dropout(0.5))\n",
    "    model2.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model2.add(Dense(nb_classes, activation='softmax'))\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    model_dir = 'C:/Users/ICT01_09/Documents/model'\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_path2 = model_dir + '/multi_img_classification2.model'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path2 , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6)   # 과적합된다는 신호가 오는데 6번정도오면 끊는다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(X_train, y_train, batch_size=32, epochs=50, validation_split=0.2, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(X_test, y_test)  # 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()  # x축은 에포크수, y 축은 손실값 \n",
    "acc_ax = loss_ax.twinx()   \n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss') # 훈련 손실값 (노란색)\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss') #검증(val=validation) 손실값( 빨간색)\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['accuracy'], 'b', label='train acc') #훈련 정확도 ( 파란색)\n",
    "acc_ax.plot(history.history['val_accuracy'], 'g', label='val acc')# 검증정확도(녹색) \n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"C:/Users/ICT01_09/Documents/train/other_test\"\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "X = []\n",
    "filenames = []\n",
    "files = glob.glob(test_dir+\"/*.*\") # 모든 파일이 출력된다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, f in enumerate(files):  \n",
    "    img = Image.open(f)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((image_w, image_h))\n",
    "    data = np.asarray(img)\n",
    "    print(data)\n",
    "    filenames.append(f)\n",
    "    \n",
    "    X.append(data)\n",
    "\n",
    "X= np.array(X)\n",
    "model2 = load_model(\"./model/multi_img_classification2.model\")#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction2 = model2.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)}) #>> 넘파이 출력옵션 변경하는것! (소수점3자리까지)\n",
    "cnt = 0\n",
    "for i in prediction2:\n",
    "    pre_ans = i.argmax()  # 예측 레이블  # argmax : 함수를 최대로 만들기 위한 x 값 --> 즉 첫번째에서는 3번째만 1이므로 2가 출력됨\n",
    "    print(i)\n",
    "    print(pre_ans)\n",
    "    pre_ans_str = ''\n",
    "    if pre_ans == 0: pre_ans_str = \"캔\"\n",
    "    elif pre_ans == 1: pre_ans_str = \"플라스틱\"\n",
    "    elif pre_ans == 2: pre_ans_str = \"유리\"\n",
    "    else: pre_ans_str = \"스티로품\"\n",
    "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
    "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델링 3  (VGG16 모델 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 훈련된 네트워크를 사용하자\n",
    "# ( 대규모 이미지 분류 문제를 위해 대량의 데이터셋에서 미리 훈련되어 저장된 네트워크이다 )\n",
    "# imagenet 데이터셋에 네트워크를 훈련한다 (14000만개의 레이블된 이미지가 있다)\n",
    "# imagenet 데이터셋에서 훈련된 대규모 컨브넷(합성곱+풀링층+완전연결층)을 사용하자! \n",
    "\n",
    "# VGG16 = imagenet 데이터셋에 널리 사용되는 컨브넷 구조이다 (imagenet데이터셋에 훈련됨)\n",
    "# ( imagenet 에 훈련된 분류 모델 : Xception, InceptionV3, ResNet50, VGG16) \n",
    "\n",
    "#사전훈련된 네트워크 사용하는 방법 1. 특성추출/ 2. 미세조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications import imagenet_utils\n",
    "import numpy as np\n",
    "\n",
    "# 1. 모델 구성하기\n",
    "model3 = VGG16(weights='imagenet') #weights='imagenet' => 모델을 초기화할 가중치 체크포인트 지정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델 사용하기 \n",
    "\n",
    "# 임의의 이미지 불러오기\n",
    "img_path = 'C:/Users/ICT01_09/Documents/train/other_test/image5.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "xhat = image.img_to_array(img)\n",
    "xhat = np.expand_dims(xhat, axis=0) # expand_dims : 차원추가 \n",
    "xhat = preprocess_input(xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 이미지로 분류 예측하기\n",
    "yhat = model3.predict(xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 확인하기\n",
    "P = imagenet_utils.decode_predictions(yhat)\n",
    "\n",
    "for (i, (imagenetID, label, prob)) in enumerate(P[0]):\n",
    "    print(\"{}. {}: {:.2f}%\".format(i + 1, label, prob * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델링5 (conv층 5개 돌리기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with K.tf_ops.device('/device:GPU:0'):\n",
    "    model5 = Sequential()\n",
    "    \n",
    "    model5.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
    "    model5.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model5.add(Dropout(0.25))\n",
    "    \n",
    "    model5.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "    model5.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model5.add(Dropout(0.25))\n",
    "    \n",
    "    model5.add(Conv2D(128, (3,3), padding=\"same\", activation='relu'))\n",
    "    model5.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model5.add(Dropout(0.25))\n",
    "    \n",
    "    model5.add(Conv2D(256, (3,3), padding=\"same\", activation='relu'))\n",
    "    model5.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model5.add(Dropout(0.25))\n",
    "    \n",
    "    model5.add(Conv2D(512, (3,3), padding=\"same\", activation='relu'))\n",
    "    model5.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model5.add(Dropout(0.25))\n",
    "    \n",
    "    model5.add(Flatten())  # flatten 시킨다 \n",
    "    model5.add(Dense(256, activation='relu'))\n",
    "    model5.add(Dropout(0.5))\n",
    "    model5.add(Dense(nb_classes, activation='softmax'))\n",
    "    model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model_dir = 'C:/Users/ICT01_09/Documents/model'\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_path5 = model_dir + '/multi_img_classification5.model'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path5 , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6)   # 과적합된다는 신호가 오는데 6번정도오면 끊는다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "24000/24000 [==============================] - 143s 6ms/step - loss: 0.8536 - accuracy: 0.6145 - val_loss: 0.6118 - val_accuracy: 0.7248\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61181, saving model to C:/Users/ICT01_09/Documents/model/multi_img_classification5.model\n",
      "Epoch 2/50\n",
      "24000/24000 [==============================] - 139s 6ms/step - loss: 0.6123 - accuracy: 0.7401 - val_loss: 0.4956 - val_accuracy: 0.8002\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61181 to 0.49558, saving model to C:/Users/ICT01_09/Documents/model/multi_img_classification5.model\n",
      "Epoch 3/50\n",
      "24000/24000 [==============================] - 142s 6ms/step - loss: 0.5027 - accuracy: 0.7923 - val_loss: 0.4535 - val_accuracy: 0.8213\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49558 to 0.45346, saving model to C:/Users/ICT01_09/Documents/model/multi_img_classification5.model\n",
      "Epoch 4/50\n",
      "24000/24000 [==============================] - 139s 6ms/step - loss: 0.4259 - accuracy: 0.8362 - val_loss: 0.4388 - val_accuracy: 0.7770\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.45346 to 0.43883, saving model to C:/Users/ICT01_09/Documents/model/multi_img_classification5.model\n",
      "Epoch 5/50\n",
      "24000/24000 [==============================] - 139s 6ms/step - loss: 0.3685 - accuracy: 0.8584 - val_loss: 0.5021 - val_accuracy: 0.7622\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.43883\n",
      "Epoch 6/50\n",
      "24000/24000 [==============================] - 140s 6ms/step - loss: 0.3323 - accuracy: 0.8735 - val_loss: 0.3753 - val_accuracy: 0.8427\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43883 to 0.37531, saving model to C:/Users/ICT01_09/Documents/model/multi_img_classification5.model\n",
      "Epoch 7/50\n",
      "24000/24000 [==============================] - 141s 6ms/step - loss: 0.2999 - accuracy: 0.8874 - val_loss: 0.3106 - val_accuracy: 0.8787\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.37531 to 0.31061, saving model to C:/Users/ICT01_09/Documents/model/multi_img_classification5.model\n",
      "Epoch 8/50\n",
      "24000/24000 [==============================] - 146s 6ms/step - loss: 0.2616 - accuracy: 0.9045 - val_loss: 0.3303 - val_accuracy: 0.8590\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.31061\n",
      "Epoch 9/50\n",
      "24000/24000 [==============================] - 139s 6ms/step - loss: 0.2506 - accuracy: 0.9087 - val_loss: 0.3167 - val_accuracy: 0.8850\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31061\n",
      "Epoch 10/50\n",
      "24000/24000 [==============================] - 138s 6ms/step - loss: 0.2387 - accuracy: 0.9157 - val_loss: 0.2704 - val_accuracy: 0.8992\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.31061 to 0.27036, saving model to C:/Users/ICT01_09/Documents/model/multi_img_classification5.model\n",
      "Epoch 11/50\n",
      "24000/24000 [==============================] - 140s 6ms/step - loss: 0.2182 - accuracy: 0.9222 - val_loss: 0.3162 - val_accuracy: 0.8667\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.27036\n",
      "Epoch 12/50\n",
      "24000/24000 [==============================] - 141s 6ms/step - loss: 0.2182 - accuracy: 0.9234 - val_loss: 0.2062 - val_accuracy: 0.9253\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.27036 to 0.20619, saving model to C:/Users/ICT01_09/Documents/model/multi_img_classification5.model\n",
      "Epoch 13/50\n",
      "24000/24000 [==============================] - 140s 6ms/step - loss: 0.1991 - accuracy: 0.9320 - val_loss: 0.2663 - val_accuracy: 0.8897\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.20619\n",
      "Epoch 14/50\n",
      "24000/24000 [==============================] - 139s 6ms/step - loss: 0.1947 - accuracy: 0.9334 - val_loss: 0.2904 - val_accuracy: 0.8558\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.20619\n",
      "Epoch 15/50\n",
      "24000/24000 [==============================] - 138s 6ms/step - loss: 0.1916 - accuracy: 0.9338 - val_loss: 0.2285 - val_accuracy: 0.9227\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.20619\n",
      "Epoch 16/50\n",
      "24000/24000 [==============================] - 139s 6ms/step - loss: 0.1853 - accuracy: 0.9362 - val_loss: 0.1867 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.20619 to 0.18675, saving model to C:/Users/ICT01_09/Documents/model/multi_img_classification5.model\n",
      "Epoch 17/50\n",
      "24000/24000 [==============================] - 139s 6ms/step - loss: 0.1706 - accuracy: 0.9413 - val_loss: 0.3293 - val_accuracy: 0.8770\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.18675\n",
      "Epoch 18/50\n",
      "24000/24000 [==============================] - 142s 6ms/step - loss: 0.1760 - accuracy: 0.9415 - val_loss: 0.2183 - val_accuracy: 0.9135\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.18675\n",
      "Epoch 19/50\n",
      "24000/24000 [==============================] - 142s 6ms/step - loss: 0.1673 - accuracy: 0.9432 - val_loss: 0.3218 - val_accuracy: 0.9033\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.18675\n",
      "Epoch 20/50\n",
      "24000/24000 [==============================] - 139s 6ms/step - loss: 0.1594 - accuracy: 0.9458 - val_loss: 0.2587 - val_accuracy: 0.8945\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.18675\n",
      "Epoch 21/50\n",
      "24000/24000 [==============================] - 138s 6ms/step - loss: 0.1590 - accuracy: 0.9473 - val_loss: 0.1556 - val_accuracy: 0.9442\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.18675 to 0.15560, saving model to C:/Users/ICT01_09/Documents/model/multi_img_classification5.model\n",
      "Epoch 22/50\n",
      "24000/24000 [==============================] - 137s 6ms/step - loss: 0.1457 - accuracy: 0.9495 - val_loss: 0.2860 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.15560\n",
      "Epoch 23/50\n",
      "24000/24000 [==============================] - 137s 6ms/step - loss: 0.1562 - accuracy: 0.9464 - val_loss: 0.2936 - val_accuracy: 0.8912\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.15560\n",
      "Epoch 24/50\n",
      "24000/24000 [==============================] - 139s 6ms/step - loss: 0.1556 - accuracy: 0.9474 - val_loss: 0.1920 - val_accuracy: 0.9415\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.15560\n",
      "Epoch 25/50\n",
      "24000/24000 [==============================] - 141s 6ms/step - loss: 0.1505 - accuracy: 0.9495 - val_loss: 0.1737 - val_accuracy: 0.9392\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.15560\n",
      "Epoch 26/50\n",
      "24000/24000 [==============================] - 140s 6ms/step - loss: 0.1390 - accuracy: 0.9524 - val_loss: 0.2127 - val_accuracy: 0.9180\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.15560\n",
      "Epoch 27/50\n",
      "24000/24000 [==============================] - 139s 6ms/step - loss: 0.1545 - accuracy: 0.9474 - val_loss: 0.3305 - val_accuracy: 0.8715\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.15560\n"
     ]
    }
   ],
   "source": [
    "history = model5.fit(X_train, y_train, batch_size=32, epochs=50, validation_split=0.2, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"C:/Users/ICT01_09/Documents/train/other_test\"\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "X = []\n",
    "filenames = []\n",
    "files = glob.glob(test_dir+\"/*.*\") # 모든 파일이 출력된다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[201 193 191]\n",
      "  [198 190 188]\n",
      "  [199 191 189]\n",
      "  ...\n",
      "  [160 142 128]\n",
      "  [156 138 124]\n",
      "  [151 133 119]]\n",
      "\n",
      " [[205 197 195]\n",
      "  [202 194 192]\n",
      "  [201 193 191]\n",
      "  ...\n",
      "  [160 142 128]\n",
      "  [156 138 124]\n",
      "  [151 133 119]]\n",
      "\n",
      " [[205 197 195]\n",
      "  [203 195 193]\n",
      "  [201 193 191]\n",
      "  ...\n",
      "  [161 143 129]\n",
      "  [157 139 125]\n",
      "  [152 134 120]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[206 206 208]\n",
      "  [206 206 208]\n",
      "  [203 203 205]\n",
      "  ...\n",
      "  [185 180 177]\n",
      "  [185 180 177]\n",
      "  [185 180 177]]\n",
      "\n",
      " [[205 205 207]\n",
      "  [205 205 207]\n",
      "  [203 203 205]\n",
      "  ...\n",
      "  [187 182 179]\n",
      "  [187 182 179]\n",
      "  [187 182 179]]\n",
      "\n",
      " [[203 203 205]\n",
      "  [203 203 205]\n",
      "  [203 203 205]\n",
      "  ...\n",
      "  [190 185 182]\n",
      "  [190 185 182]\n",
      "  [190 185 182]]]\n",
      "[[[124 112  90]\n",
      "  [128 116  94]\n",
      "  [129 117  95]\n",
      "  ...\n",
      "  [130 150  63]\n",
      "  [127 147  62]\n",
      "  [127 147  60]]\n",
      "\n",
      " [[131 118  99]\n",
      "  [139 126 109]\n",
      "  [133 121  99]\n",
      "  ...\n",
      "  [127 150  62]\n",
      "  [127 147  62]\n",
      "  [126 146  59]]\n",
      "\n",
      " [[142 133 118]\n",
      "  [158 148 138]\n",
      "  [145 133 117]\n",
      "  ...\n",
      "  [129 149  64]\n",
      "  [128 148  63]\n",
      "  [126 146  61]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[168 165 160]\n",
      "  [169 164 160]\n",
      "  [170 167 162]\n",
      "  ...\n",
      "  [132  96  82]\n",
      "  [136 100  88]\n",
      "  [133  97  83]]\n",
      "\n",
      " [[169 164 160]\n",
      "  [169 164 160]\n",
      "  [170 165 161]\n",
      "  ...\n",
      "  [130  97  82]\n",
      "  [138 104  92]\n",
      "  [134  98  84]]\n",
      "\n",
      " [[168 163 159]\n",
      "  [168 163 159]\n",
      "  [170 165 161]\n",
      "  ...\n",
      "  [132  96  82]\n",
      "  [138 105  90]\n",
      "  [131  95  83]]]\n",
      "[[[ 77  54  22]\n",
      "  [ 82  62  25]\n",
      "  [179 159 122]\n",
      "  ...\n",
      "  [ 94  70  34]\n",
      "  [ 92  69  35]\n",
      "  [ 90  68  31]]\n",
      "\n",
      " [[ 86  61  30]\n",
      "  [135 116  84]\n",
      "  [176 158 118]\n",
      "  ...\n",
      "  [ 92  67  36]\n",
      "  [ 94  70  36]\n",
      "  [ 92  66  31]]\n",
      "\n",
      " [[ 88  66  29]\n",
      "  [105 102  83]\n",
      "  [163 143 106]\n",
      "  ...\n",
      "  [ 98  72  37]\n",
      "  [103  75  38]\n",
      "  [102  74  35]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 87  55  16]\n",
      "  [ 90  56  18]\n",
      "  [102  64  17]\n",
      "  ...\n",
      "  [140  97  46]\n",
      "  [134  90  41]\n",
      "  [122  85  41]]\n",
      "\n",
      " [[ 96  59  17]\n",
      "  [100  59  15]\n",
      "  [113  72  26]\n",
      "  ...\n",
      "  [122  84  39]\n",
      "  [124  87  42]\n",
      "  [118  83  43]]\n",
      "\n",
      " [[101  59  17]\n",
      "  [ 89  55  10]\n",
      "  [ 93  53   4]\n",
      "  ...\n",
      "  [114  79  37]\n",
      "  [114  79  41]\n",
      "  [122  85  40]]]\n",
      "[[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n",
      "[[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [254 255 255]\n",
      "  ...\n",
      "  [254 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 253]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [254 255 255]\n",
      "  ...\n",
      "  [254 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 253]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [254 255 255]\n",
      "  ...\n",
      "  [254 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 253]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 253]\n",
      "  [255 255 253]\n",
      "  [255 255 253]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 253]\n",
      "  [255 255 253]\n",
      "  [255 255 253]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 253]\n",
      "  [255 255 253]\n",
      "  [255 255 253]]]\n",
      "[[[160 153 147]\n",
      "  [161 154 148]\n",
      "  [161 154 148]\n",
      "  ...\n",
      "  [168 163 160]\n",
      "  [168 163 160]\n",
      "  [168 163 160]]\n",
      "\n",
      " [[155 148 142]\n",
      "  [156 149 143]\n",
      "  [156 149 143]\n",
      "  ...\n",
      "  [168 163 160]\n",
      "  [168 163 160]\n",
      "  [168 163 160]]\n",
      "\n",
      " [[150 143 137]\n",
      "  [151 144 138]\n",
      "  [152 145 139]\n",
      "  ...\n",
      "  [167 162 159]\n",
      "  [167 162 159]\n",
      "  [167 162 159]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[210 206 197]\n",
      "  [210 206 197]\n",
      "  [210 206 197]\n",
      "  ...\n",
      "  [213 206 200]\n",
      "  [213 206 200]\n",
      "  [213 206 200]]\n",
      "\n",
      " [[210 206 197]\n",
      "  [210 206 197]\n",
      "  [211 207 198]\n",
      "  ...\n",
      "  [212 205 199]\n",
      "  [212 205 199]\n",
      "  [212 205 199]]\n",
      "\n",
      " [[210 206 197]\n",
      "  [210 206 197]\n",
      "  [212 208 199]\n",
      "  ...\n",
      "  [210 203 197]\n",
      "  [210 203 197]\n",
      "  [210 203 197]]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-59ff9ae4deaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./model/multi_img_classification5.model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(files):  \n",
    "    img = Image.open(f)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((image_w, image_h))\n",
    "    data = np.asarray(img)\n",
    "    print(data)\n",
    "    filenames.append(f)\n",
    "    \n",
    "    X.append(data)\n",
    "\n",
    "X= np.array(X)\n",
    "model5 = load_model(\"./model/multi_img_classification5.model\")#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction5 = model5.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.000 0.000 0.000 0.000]\n",
      "0\n",
      "해당 imag3.jpg이미지는 캔로 추정됩니다.\n",
      "[1.000 0.000 0.000 0.000]\n",
      "0\n",
      "해당 image1.jpg이미지는 캔로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000]\n",
      "2\n",
      "해당 image2.jpg이미지는 유리로 추정됩니다.\n",
      "[1.000 0.000 0.000 0.000]\n",
      "0\n",
      "해당 image4.jpg이미지는 캔로 추정됩니다.\n",
      "[1.000 0.000 0.000 0.000]\n",
      "0\n",
      "해당 image5.jpg이미지는 캔로 추정됩니다.\n",
      "[1.000 0.000 0.000 0.000]\n",
      "0\n",
      "해당 images.jpg이미지는 캔로 추정됩니다.\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)}) #>> 넘파이 출력옵션 변경하는것! (소수점3자리까지)\n",
    "cnt = 0\n",
    "for i in prediction5:\n",
    "    pre_ans = i.argmax()  # 예측 레이블  # argmax : 함수를 최대로 만들기 위한 x 값 --> 즉 첫번째에서는 3번째만 1이므로 2가 출력됨\n",
    "    print(i)\n",
    "    print(pre_ans)\n",
    "    pre_ans_str = ''\n",
    "    if pre_ans == 0: pre_ans_str = \"캔\"\n",
    "    elif pre_ans == 1: pre_ans_str = \"플라스틱\"\n",
    "    elif pre_ans == 2: pre_ans_str = \"유리\"\n",
    "    else: pre_ans_str = \"스티로품\"\n",
    "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
    "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델링2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), input_shape=X_train.shape[1:],padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(nb_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " optimizer='adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,epochs=30,batch_size=32, epochs=50,validation_split=0.2,\n",
    "         callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델링1 : 간단한 컨브넷 만들기( conv2d, maxpooling2d 층을 쌓아올렸다) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3,3), input_shape=(64,64,3), activation='relu'))\n",
    "# -> input 으로 64x64x3 이 들어간다.(input height, input width, input channel=색)  \n",
    "# -> 3x3 크기의 필터 32개를 사용한다.\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "model.add(layers.Conv2D(128,(3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2d, Maxpooling2D 의 출력은 (height, width, channels ) \n",
    "# 마지막 층의 (12,12,64) 를 완전 연결층에 주입한다 ( flatten 시켜서 3d 출력을 1d 텐서로 만든다 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(128,activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(4, activation='softmax')) # 4개의 클래스로 분류해보기 위해 마지막 층의 출력 크기를 4로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.summary() #>> (12,12,64) 가 (9216,) 크기의 벡터로 펼쳐진후 dense 층에 주입되었음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##훈련+ 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load('./multi_image_data.npy',allow_pickle=True)\n",
    "print(X_train.shape)\n",
    "print(X_train.shape[0])\n",
    "print(X_test.shape)\n",
    "#print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')/255 \n",
    "X_test = X_test.astype('float32')/ 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = tf.reshape(X_train,[-1,64,64,1])\n",
    "\n",
    "#X_test = tf.reshape(X_test,[-1,64,64,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train,4) # 활성화함수를 적용하기 위해 원핫인코딩한다고 생각하면 될듯 \n",
    "y_test = to_categorical(y_test,4)\n",
    "#print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#신경망의 성능을 개선하기 위해 하는 것\n",
    "#https://datascienceschool.net/view-notebook/f18248a467e94c6483783afc93d08af9/\n",
    "# https://tykimos.github.io/2017/09/24/Custom_Metric/\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#1.  RMSprop 옵티마이저 사용\n",
    "#2.  loss인수를 설정하여 크로스 엔트로피 사용\n",
    "#3.  metrics -> 평가기준을 말한다 (학습과정중 제대로학습하는지) ,일반적으로 accuracy 를 삽입한다 )\n",
    "#   --> 내부적으로 categorical_accuracy 함수를 이용하여 정확도 계싼이 가능하다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train[:,:,-1],y_train[:,:,-1], steps_per_epoch=5,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
    "# 3x3 크기의 필터 32개 사용 , 활성화 함수는 relu 를 사용 ( input 으로는 64x64x3 이들어감)\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Conv2D(64,(3,3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 완전연결층 \n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('정확도 : %.4f' % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with K.tf_ops.device('/device:GPU:0'): # 디바이스 현재 GPU 없이 CPU로 구동중\n",
    "    model = Sequential() \n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
    "    # 3x3 크기의 필터 32개 사용 , 활성화 함수는 relu 를 사용 ( input 으로는 64x64x3 이들어감)\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64,(3,3),padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # 완전연결층 \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model_dir = './model'\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_path = model_dir + '/multi_img_classification.model'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2= model\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "def compile_and_train(model, num_epochs):\n",
    "    model.compile(loss = categorical_crossentropy, optimizer=Adam(), metrics=['acc'])\n",
    "    history= model.fit(x=X_train, y=y_train, batch_size=32, epochs=num_epochs, verbose=1,\n",
    "                       validation_split =0.2) \n",
    "\n",
    "    \n",
    "_ = compile_and_train(model2, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"정확도 : %.4f\" % (model2.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "def evaluate_error(model):\n",
    "    pred = model.predict(X_test, batch_size=32)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    pred = np.expand_dims(pred, axis=1) # make same shape as y_test\n",
    "    error = np.sum(np.not_equal(pred, y_test))/ y_test.shape[0]\n",
    "    return error\n",
    "\n",
    "\n",
    "\n",
    "evaluate_error(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(맨 앞부분에서 해보기)\n",
    "# generator -> 사진크기 늘리기 ( )\n",
    "# 정규화"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "192.358px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
