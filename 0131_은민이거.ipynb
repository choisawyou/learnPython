{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://blog.naver.com/businessinsight/221789046221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://wikidocs.net/47193"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 뉴런 : 뽑아내는 특징수 = hidden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지금까지 FFNN 과 cNN 을 했다   ( CNN으로 특징을 추출하고나서 그 뒤에 FFNN 이 붙는다)\n",
    "# RNN 도 마찬가지다 (RNN 으로 특징을 추출하고, FFNN 이 뒤에 붙는다)\n",
    "# 모든망의 기본은 FFNN 이다 \n",
    "\n",
    "# FFNN  : 입력데이터가 들어간다   -> 가중치 이용해서 특징을 추출한다 \n",
    "#        ( 행을 여러개 묶은것을 minibatch, 각각하나를 data point, )\n",
    "\n",
    "#RNN : recurrent neural network\n",
    "# RNN 순서가 있는 데이터(시계열 데이터, 연속적데이터 등) 를 다룰수 있다 \n",
    "#RNN 에서는    입력데이터 들어와서 가중치랑 계산하고 output 하는 과정을 cell 이라고 한다. 하나의 cell 을 여러개를 묶어서 연결시킨다\n",
    " # 다음 셀에서 가중치를 계산할때는 이전 셀의 가중치의 영향력을 고려한다  ( 앞단의 데이터 계산이 먼저 끝나야한다  -> latent time ( 시간 지연이 발생한다 ) \n",
    "    \n",
    "    \n",
    "# simple RNN 에서 나온 것 : LSTM ( long short time memory)\n",
    "# LSTM 은 처음의 영향력을 놓치지 않기위해서  control state 와 hidden state 를 만듬 ->복잡한계산->( hidden state 가 output 이다 / control state : 이전셀에 있던 망 (다음셀에 영향을 미칠 것을 계산함)  ) \n",
    "# LSTM 은 gate 가 3개가 있다 ( forget, input, output) \n",
    "# forget : 앞단에서 흘러들어오는 것(앞단에서 들어오는 값을) 을 어떻게 제어할것인지 (망각할지 유지할지)\n",
    "# input : 다음데이터가 control 선에 어떻게 영향을미칠지 control 선을 제어하는 역할을 한다\n",
    "# LSTM 에서는 가중치가 4개가 필요하다 \n",
    "#RNN 하다보면 첫번째껏이 영향력을 잃기때문에 그것을 잃지 않게 하기 위해서 control 제어선을 통해서 장기기억력을 가짐 (LSTM)\n",
    "# RNN 에는 control 제어선이 없었다 \n",
    "\n",
    "\n",
    "# GRU  : LSTM 을 좀 간단하게 한 것이다, 회로를 간략하게 해두었다 ( 연산이 줄어들었음 ) \n",
    "\n",
    "# https://wikidocs.net/22888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캡셔닝?? !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one to many ( 하나의 입력으로 여러개의 결과가 나온다. 캡셔닝할때 사용)\n",
    "# mamy to one : 여러개들어가서 한개가나옴 ( 감정분류할떄 사용)\n",
    "# mamy to many 번역할때 사용\n",
    "# many to many  : 동영상 frame 에서 객체를 detect 하는 것\n",
    "# 사진 참고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇 (ex. 기분에 따라 밥 추천,  카카오나 텔레그램 같은 서버만 있으면 챗봇망 만들수있다 ) 챗봇은 RNN 의 원리라고 생각하자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  simple RNN \n",
    "# bi-directional RNN : 한쪽방향으로 특징을 뽑아도 좋지만, 한쪽방향뿐만아니라 양쪽방향으로 특징을 뽑는다  \n",
    "# multi RNN\n",
    "# 사진 ( 순서대로 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 들어오는 데이터도 수치, hidden layer 도 수치, output lyaer 에서도 수치가 나온다  ( 사진) \n",
    "# 사진을 보면 hidden lyaer 가 다음 망에 또 영향을 주는 것을 확인할 수 있다 \n",
    "\n",
    "\n",
    "# sequence to sequence 라는 유명한 번역망이 있다. ( sequence 가 들어오면 sequence 가 나온다 ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static Rnn(cell 수가 고정) . 다이나믹rnn (시간적으로 계산돼서 늘어져있는?? ) 이 있다  -> 뭔지 모르겠음\n",
    "\n",
    "# https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A1E05AC08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A1E05AC08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A1E05AC08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A1E05AC08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A1E05AC08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A1E05AC08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A1E05AC08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A1E05AC08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()     # 그라프 초기화 ( 초기화안하면 같은 이름이 있다고 뜸)\n",
    "\n",
    "n_inputs = 3  # 입력데이터\n",
    "n_neurons=5  # 셀의 가중치 사이즈  (특성을 찾아내는 수 ) \n",
    "\n",
    "# 데이터 받기위해 placeholder 지정\n",
    "X0= tf.placeholder(tf.float32, [None, n_inputs])   #>> 4x3 으로 들어온다 (none=4)\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])  # >> 4x3 으로 들어온다 \n",
    "#[0,1,2] 사이즈가 하나의 셀로 입장한다\n",
    "\n",
    "# 말단 데이터 하나가 FFNN 이라고 했다 \n",
    "\n",
    "# 입력하기위해서 basic_Cell 을 만들었다 \n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) # 가중치 사이즈를 말한다 (가중치 다른말로하면 특성을 찾아내도록 돕는것)\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [X0, X1], dtype=tf.float32)  # static_rnn : RNN 네트워크\n",
    "                                              # x0,x1 이 들어오면 2x4x3  >>>>>  자동으로 4개의 셀이 연결되면서 메모리 확보 \n",
    "                                                #(2는 순차적으로 들어오는 수 라고보자)  즉 배치사이즈(2)x 셀수(4)x 뉴런수(3) 로 이해하자  \n",
    "\n",
    "        \n",
    "        \n",
    "# RNN 에서는  출력되는 값이 있고, 다음셀로 전달되는 값이 있다 >>states : 다음으로 연결되어질 값\n",
    "# 수평으로 셀을 연결해주는 값이 있는데 마지막에 있는 놈이 state 값으로 나오게 된다 (?)\n",
    "Y0,Y1= output_seqs\n",
    "init = tf.global_variables_initializer()\n",
    "X0_batch = np.array([[0,1,2],[3,4,5], [6,7,8], [9,0,1]])  # 이건 플레이스홀더로 받을 데이터 \n",
    "X1_batch = np.array([[9,8,7], [0,0,0],[6,5,4], [3,2,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처음 데이터 특성: [[-0.00905584 -0.4969326   0.87278956  0.28922752 -0.8220851 ]\n",
      " [ 0.6645853  -0.68588114  0.99401236 -0.57975537 -0.9998801 ]\n",
      " [ 0.9233105  -0.81273985  0.99973446 -0.924901   -0.9999999 ]\n",
      " [ 0.9999152  -0.16552094 -0.7106973  -0.9965082  -0.99999714]] 차수: (4, 5)\n",
      "두번째 데이터 특성: [[ 0.96326524 -0.35662225  0.996694   -0.98327196 -1.        ]\n",
      " [-0.5812175   0.19084631  0.03750269  0.18285428  0.2797232 ]\n",
      " [ 0.71750855  0.17392094  0.9586217  -0.9806219  -0.99998224]\n",
      " [ 0.84901255  0.82165736 -0.19268803 -0.9656343  -0.9778151 ]] 차수: (4, 5)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0,Y1], feed_dict={X0:X0_batch, X1:X1_batch})\n",
    "                                                            # 4x3 에서 4 x 5  로 변햿다 (n_neuros 때문에 5개가 된것)\n",
    "print('처음 데이터 특성:', Y0_val, \"차수:\",Y0_val.shape) # y0 를 출력했더니 4x5 가 되었다 (위에 4개의 셀이 있으므로 4이다(X0_batch 보면4개) ,특징이 3개에서 5개로 변함)\n",
    "print('두번째 데이터 특성:', Y1_val, '차수:',Y1_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀이 가지고 있는 가중치 사이즈는???   3x5 이다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CMM ( 주변고려해서 특징 추출), RNN (순차성을 가진 데이터의 특성을 잡아냄 )_ >> 둘다 특징추출로 FFNN 전 단계에서 사용한다 \n",
    "# 이미지도 RNN 으로 잡아낼수있다\n",
    "#CNN 도 텍스트 마이닝 을 하고  요새는 복합적으로 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A20633F88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A20633F88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A20633F88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A20633F88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000015A206B77C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000015A206B77C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000015A206B77C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000015A206B77C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "# 원래이미지는 784(28x28) 이였는데\n",
    "tf.reset_default_graph()\n",
    "n_steps=28  # 쎌이 28개  \n",
    "n_inputs=28  # 셀당 input size \n",
    "n_neurons=150 # 뉴런이 150개\n",
    "n_outputs =10 # output 이 10개 ( accuracy 즉 확률사이즈 ) \n",
    "learning_rate = 0.001\n",
    "                                     # n_steps: 배치사이즈, # n_inputs : 셀당 입력사이즈 \n",
    "X= tf.placeholder(tf.float32, [None, n_steps, n_inputs])  # 아까는 2차원인데 지금은 3차원이다 (아까는 직접데이터를 입력했기때문, 지금은 3차원데이터를 가져오기때문이다) \n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) # 150개 만들어진다 \n",
    "#>>>  셀하나를 만들면 그 셀하나가 FFNN 이다 ( 사진) \n",
    "# 셀에 들어가는 데이터가 28개다 그리고 output 되는 뉴런수가 150개 이다. 그럼 그 가중치는 28x150 이 되어야한다 .(셀이들어와서 가중치랑 계싼되서 output됨) \n",
    "# 근데 셀이 쭈욱 28개가 만들어진다 (28개 셀생성) (옆으로 나열된다고 생각하자, 이전셀을 고려해서 다음셀이 진행됨) \n",
    "# 그리고 28개의 셀을 지나서 마지막으로 나가는 놈이 states 이다 . ( 앞에있는 셀들의 특징도 전부 고려된것이다 ) \n",
    "# 배치사이즈가 150개니까 states 가 150개가 만들어짐 \n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32) # basic_cell 을 주고 데이터 입력받는다(X) (데이터가 3차원으로 입력받아야한다 )  \n",
    "#                  >> 자동으로 2차원으로 만들어진다 ( 셀이 28개가 만들어진다, )\n",
    " # states 가 의미하는 것 : 마지막 셀의 수평으로 전달되는 값이다 \n",
    "    # 마지막 state 는 output 과 같다  (state 는 계속 나오는것) \n",
    "    ## 28개의 셀이 있는데 마지막 1개의 output 을 사용하는 것 -> many to one 망을 사용하는 것이다!! ( 감정분류같은  분류기에 사용됨)\n",
    "    # state 의 차수는 150(배치가150) x  150 ( 마지막셀에서 나온 뉴런수)\n",
    "    # outputs ->>> 각각 latent time ( 지연시간) 을 통해서 계산되어진 셀의 모든 값을 결합해서 출력한다 --> 150x28(셀수)x 150 (t셀당150개 뉴런출력) \n",
    "# 메모장 참고\n",
    "\n",
    "\n",
    "# 150개의 특징중에서 10개만 추출   \n",
    "# dense 에 입력차수와 출력차수를 지정하면 자동으로 bias 를 만들고 가중치 공간을 확보해준다\n",
    "\n",
    "logits= tf.layers.dense(states, n_outputs) # states 가 150x150 으로 나왔고, outputs 이 150x10으로 나가주어야한다  >> 따라서 dense 의 가중치 사이즈는 150x 10 이 된다 \n",
    "# 150x 10 으로 예측된 logits\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) # 분류를 위한 미분이 가능한 식으로 바뀐다 \n",
    "\n",
    "loss= tf.reduce_mean(xentropy)  # 150개 나온것들의 평균을 내준다 =loss    (배치사이즈를 쓰는(평균을 사용하는) 이유: 지역해에 빠지는것을 방지)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate) # adam 은 momentum + propgrad를 합쳐놓은 것\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct= tf.nn.in_top_k(logits,y, 1) # in_top_k: 양쪽에 큰놈을 해서 비교한다 즉 가장큰값을 취해서 비교한다 ( 원래는 argmax 썼었음) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # correct 된 값을 reduce_mean 하니까 accuracy 가 된다 \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-9aff0e2313ad>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_01\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_01\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_01\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_01\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")  # 데이터 인풋 하고 \n",
    "X_test = mnist.test.images.reshape((-1, n_steps, n_inputs)) # reshape 시켜준다 (  원래이미지 모양으로 학습한다는 뜻)\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.91333336 Test accuracy: 0.9228\n",
      "1 Train accuracy: 0.96666664 Test accuracy: 0.9461\n",
      "2 Train accuracy: 0.96 Test accuracy: 0.9566\n",
      "3 Train accuracy: 0.9533333 Test accuracy: 0.9476\n",
      "4 Train accuracy: 0.96 Test accuracy: 0.9703\n",
      "5 Train accuracy: 0.9866667 Test accuracy: 0.9615\n",
      "6 Train accuracy: 0.94666666 Test accuracy: 0.9642\n",
      "7 Train accuracy: 0.98 Test accuracy: 0.9727\n",
      "8 Train accuracy: 0.99333334 Test accuracy: 0.9707\n",
      "9 Train accuracy: 0.97333336 Test accuracy: 0.9715\n",
      "10 Train accuracy: 0.96666664 Test accuracy: 0.9652\n",
      "11 Train accuracy: 0.96666664 Test accuracy: 0.9726\n",
      "12 Train accuracy: 0.97333336 Test accuracy: 0.9681\n",
      "13 Train accuracy: 0.99333334 Test accuracy: 0.976\n",
      "14 Train accuracy: 0.98 Test accuracy: 0.976\n",
      "15 Train accuracy: 0.9866667 Test accuracy: 0.9761\n",
      "16 Train accuracy: 1.0 Test accuracy: 0.9701\n",
      "17 Train accuracy: 0.98 Test accuracy: 0.9738\n",
      "18 Train accuracy: 1.0 Test accuracy: 0.9703\n",
      "19 Train accuracy: 1.0 Test accuracy: 0.9754\n",
      "20 Train accuracy: 1.0 Test accuracy: 0.9779\n",
      "21 Train accuracy: 0.99333334 Test accuracy: 0.9768\n",
      "22 Train accuracy: 1.0 Test accuracy: 0.9763\n",
      "23 Train accuracy: 0.98 Test accuracy: 0.9659\n",
      "24 Train accuracy: 0.9866667 Test accuracy: 0.9781\n",
      "25 Train accuracy: 0.99333334 Test accuracy: 0.9729\n",
      "26 Train accuracy: 1.0 Test accuracy: 0.9771\n",
      "27 Train accuracy: 0.9866667 Test accuracy: 0.9777\n",
      "28 Train accuracy: 0.99333334 Test accuracy: 0.9749\n",
      "29 Train accuracy: 0.99333334 Test accuracy: 0.9803\n",
      "30 Train accuracy: 0.99333334 Test accuracy: 0.9794\n",
      "31 Train accuracy: 0.99333334 Test accuracy: 0.9737\n",
      "32 Train accuracy: 0.98 Test accuracy: 0.9697\n",
      "33 Train accuracy: 0.99333334 Test accuracy: 0.9776\n",
      "34 Train accuracy: 1.0 Test accuracy: 0.9793\n",
      "35 Train accuracy: 0.9866667 Test accuracy: 0.9796\n",
      "36 Train accuracy: 0.97333336 Test accuracy: 0.9769\n",
      "37 Train accuracy: 1.0 Test accuracy: 0.9741\n",
      "38 Train accuracy: 1.0 Test accuracy: 0.9751\n",
      "39 Train accuracy: 1.0 Test accuracy: 0.9796\n",
      "40 Train accuracy: 0.9866667 Test accuracy: 0.9773\n",
      "41 Train accuracy: 1.0 Test accuracy: 0.9758\n",
      "42 Train accuracy: 1.0 Test accuracy: 0.9745\n",
      "43 Train accuracy: 0.99333334 Test accuracy: 0.9791\n",
      "44 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "45 Train accuracy: 0.9866667 Test accuracy: 0.9787\n",
      "46 Train accuracy: 0.99333334 Test accuracy: 0.9705\n",
      "47 Train accuracy: 0.99333334 Test accuracy: 0.9792\n",
      "48 Train accuracy: 1.0 Test accuracy: 0.9789\n",
      "49 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "50 Train accuracy: 0.99333334 Test accuracy: 0.9764\n",
      "51 Train accuracy: 1.0 Test accuracy: 0.9774\n",
      "52 Train accuracy: 1.0 Test accuracy: 0.9754\n",
      "53 Train accuracy: 0.9866667 Test accuracy: 0.9754\n",
      "54 Train accuracy: 0.98 Test accuracy: 0.9747\n",
      "55 Train accuracy: 0.98 Test accuracy: 0.9699\n",
      "56 Train accuracy: 0.9866667 Test accuracy: 0.9787\n",
      "57 Train accuracy: 0.98 Test accuracy: 0.9768\n",
      "58 Train accuracy: 0.99333334 Test accuracy: 0.9725\n",
      "59 Train accuracy: 0.9866667 Test accuracy: 0.9787\n",
      "60 Train accuracy: 1.0 Test accuracy: 0.9729\n",
      "61 Train accuracy: 1.0 Test accuracy: 0.9778\n",
      "62 Train accuracy: 0.9866667 Test accuracy: 0.9727\n",
      "63 Train accuracy: 0.99333334 Test accuracy: 0.9754\n",
      "64 Train accuracy: 1.0 Test accuracy: 0.9769\n",
      "65 Train accuracy: 1.0 Test accuracy: 0.9755\n",
      "66 Train accuracy: 0.99333334 Test accuracy: 0.9797\n",
      "67 Train accuracy: 1.0 Test accuracy: 0.9764\n",
      "68 Train accuracy: 0.99333334 Test accuracy: 0.9788\n",
      "69 Train accuracy: 0.99333334 Test accuracy: 0.9781\n",
      "70 Train accuracy: 1.0 Test accuracy: 0.9753\n",
      "71 Train accuracy: 0.9866667 Test accuracy: 0.9736\n",
      "72 Train accuracy: 0.99333334 Test accuracy: 0.9776\n",
      "73 Train accuracy: 1.0 Test accuracy: 0.9796\n",
      "74 Train accuracy: 1.0 Test accuracy: 0.9773\n",
      "75 Train accuracy: 1.0 Test accuracy: 0.9777\n",
      "76 Train accuracy: 0.99333334 Test accuracy: 0.9792\n",
      "77 Train accuracy: 0.99333334 Test accuracy: 0.9785\n",
      "78 Train accuracy: 0.9866667 Test accuracy: 0.9766\n",
      "79 Train accuracy: 0.98 Test accuracy: 0.9709\n",
      "80 Train accuracy: 0.99333334 Test accuracy: 0.9762\n",
      "81 Train accuracy: 0.99333334 Test accuracy: 0.9781\n",
      "82 Train accuracy: 0.9866667 Test accuracy: 0.9798\n",
      "83 Train accuracy: 0.99333334 Test accuracy: 0.9806\n",
      "84 Train accuracy: 0.99333334 Test accuracy: 0.9785\n",
      "85 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "86 Train accuracy: 0.99333334 Test accuracy: 0.9747\n",
      "87 Train accuracy: 0.9866667 Test accuracy: 0.9815\n",
      "88 Train accuracy: 1.0 Test accuracy: 0.9771\n",
      "89 Train accuracy: 0.9866667 Test accuracy: 0.9779\n",
      "90 Train accuracy: 1.0 Test accuracy: 0.9774\n",
      "91 Train accuracy: 1.0 Test accuracy: 0.9779\n",
      "92 Train accuracy: 0.9866667 Test accuracy: 0.9799\n",
      "93 Train accuracy: 0.9866667 Test accuracy: 0.9757\n",
      "94 Train accuracy: 0.96 Test accuracy: 0.9759\n",
      "95 Train accuracy: 0.99333334 Test accuracy: 0.975\n",
      "96 Train accuracy: 0.9866667 Test accuracy: 0.9763\n",
      "97 Train accuracy: 0.9866667 Test accuracy: 0.9794\n",
      "98 Train accuracy: 1.0 Test accuracy: 0.9786\n",
      "99 Train accuracy: 0.9866667 Test accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size=150\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):  # n_epochs 만큼 돌아간다 \n",
    "        for iteration in range(mnist.train.num_examples // batch_size): # 배치사이즈로 나누니까>>> 60000개의 데이터/150개의 배치사이즈 로 나눈값만큼 돌고나서 또 epoch 만큼 돈다 \n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            # 이미지 사이즈로 생성 \n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict = {X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict = {X: X_test, y:y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test) # 98%정도 나온다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MUltiRNNCELll 을 할것이다 >> 3단계로 구성되어있다  (basic rnncell, multi rnncell, dynamic_rnn 으로 구성)\n",
    "#BasicRNNCell : 기본데이터가 입력되는 곳은 basicrnncell 이다!! \n",
    "# MultiRNNCell : 수직으로 레이어 구성( 아까는 이거 없었음 )  수평으로 구성되어있었는데 이번에는 수직으로도 구성 >> 밑에보면 3개로\n",
    "#            >>> multirnncell 로 의해서 수직으로 셀이 구성 \n",
    "# dynamic_rnn : 이것이 모델이다 \n",
    "tf.reset_default_graph()  # \n",
    "n_steps= 28\n",
    "n_inputs=28\n",
    "n_outputs=10\n",
    "learning_rate = 0.001\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "n_neurons=100\n",
    "n_layers=3  # 3개의 멀티레이어 \n",
    "layers=[tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu) for layer in range(n_layers)]  # basicrnncell 은  3개만들어짐 for 문통해 확인가능>>> 3개의 셀 생성\n",
    "## 3개를 쌓으려면 반복해야해서 for 문을 쓴다!!! \n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers) # 3개의 셀을 조합해서 multirnncell 을 만든다 ( 셀이 3층으로 구성되어있다 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://excelsior-cjh.tistory.com/184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000015A638E3D88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000015A638E3D88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000015A638E3D88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000015A638E3D88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A24A565C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A24A565C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A24A565C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A24A565C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A24A56B88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A24A56B88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A24A56B88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A24A56B88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A24A5DD08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A24A5DD08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A24A5DD08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A24A5DD08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000015A73356148>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000015A73356148>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000015A73356148>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000015A73356148>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "# 이번엔 dynamic_rnn 이다  : 셀로입력되는 데이터의 개수에 맞추어서 셀을 구성할 수 있다 \n",
    "#ex )  나는 학교에 간다. ( 2글자 + 3글자+2글자 --> 사이즈가 다르다 -> \n",
    "#  사이즈가 다르면 static_rnn 에서는  동일한사이즈로 맞췄다.-> 큰것을 기준으로 작은 것을 padding 했었다 >> 결과가 안좋게 나오기떄문에 dynamic_rnn 을 쓴다 \n",
    "#  dynamic_rnn 은 사이즈가 다르면 다른대로 상관하지 않는다. (입력사이즈를 변동>>> 가중치를 조절한다 >> 나가는 특징은 일치함 = neural 수는 같다 ) \n",
    "\n",
    "# 3층 셀이 28개가 조성이 된다 :: 고정사이즈이다  \n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "\n",
    "# states 가 몇개가 발생하는가?? \n",
    "# 3x 150x 100  ( 층이 3층이므로 3층, 배치사이즈가 150이니까 150,   150x100 인데 3층이니까 3이붙는것이다, 100=neuron) >> 특징을 3번추출하는것\n",
    "\n",
    "states_concat = tf.concat(axis=1, values= states)  # concat 이 열방향으로 하게됨 (axis=1 에 의해서) >>> 150x300 이 된다  \n",
    "logits= tf.layers.dense(states_concat, n_outputs)  # dense : 입력차수와 출력차수를 넣어주면 가중치 사이즈를 자기가 만든다 \n",
    "                                            #150x300 이 들어오고,  출력차수가 10이니까 가중치는 300 x10 이 됨   \n",
    "# 300 x10 의 가중치학습 (ㅇ ㅕ기서부터는 FFNN 이다 ) \n",
    "\n",
    "    #가중치계산해서 나온결과는  150( 미니배치사이즈 ) x10 ( 확률적으로 나타나는 10개의 특징=확률값)\n",
    "        # 원핫인코딩 x log( 에측값)  ? ? \n",
    "    \n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,  logits=logits)\n",
    "loss= tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits,y,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.93333334 Test accuracy: 0.9259\n",
      "1 Train accuracy: 0.96666664 Test accuracy: 0.9629\n",
      "2 Train accuracy: 0.98 Test accuracy: 0.9607\n",
      "3 Train accuracy: 0.99333334 Test accuracy: 0.9728\n",
      "4 Train accuracy: 0.98 Test accuracy: 0.9768\n",
      "5 Train accuracy: 0.9866667 Test accuracy: 0.9758\n",
      "6 Train accuracy: 0.99333334 Test accuracy: 0.9719\n",
      "7 Train accuracy: 0.98 Test accuracy: 0.9768\n",
      "8 Train accuracy: 0.98 Test accuracy: 0.9779\n",
      "9 Train accuracy: 0.97333336 Test accuracy: 0.9766\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size=150\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):  \n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict = {X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict = {X: X_test, y:y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  핵심은 basic cell, dynamic_rnn 이다!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets('./mnist/data/', one_hot=True)\n",
    "learning_rate = 0.001\n",
    "total_epoch = 10\n",
    "batch_size=128\n",
    "n_input=28\n",
    "n_step=28\n",
    "n_hidden=128\n",
    "n_class=10\n",
    "X=tf.placeholder(tf.float32, [None,n_step, n_input]) ##>> X 사이즈 128x28x28  \n",
    "Y= tf.placeholder(tf.float32, [None,n_class]) # 128x10\n",
    "W= tf.Variable(tf.random_normal([n_hidden, n_class])) # 가중치사이즈는 : 128x10\n",
    "b= tf.Variable(tf.random_normal([n_class]))  # 10\n",
    " # ( 28x128 사이즈의 가중치가 자동으로 만들어진다 ) -> 입력은 28개 그리고 통과하게 되면 128개 특징이 나온다!!!\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)  # basicrnncell 이니까 neurons은 128 특징 \n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32) # dynamic_rnn 에 의해서 cell 수는 28개  \n",
    "# output 사이즈는 : 128x 28x 128  이다 ( 28개의 셀이있고, 그 셀마다 뉴런이 128개(뒷숫자), 한행이 28개 들어오면 128개(뒷숫자) 로 확장해서 특징을 뽑는다 )  맨앞은 128은 배치사이즈를 말한다  \n",
    "# states 사이즈는  128x128 이 된다 \n",
    "\n",
    "\n",
    "# 특징을 뽑았고 이제부터는 FFNN 망이다 (FFNN 에서는 128개의 특징을 10개로 )\n",
    "outputs = tf.transpose(outputs, [1,0,2]) # 128x28x128 을 transpose >>>> 28x128x128 \n",
    "outputs = outputs[-1]  # -1 하는 이유는 ? 28은 셀을 말하는건데  즉 셀로 판단하게 위해서 이러한 코드를 썻다 \n",
    "#              >>>한셀마다 128개의 특징이 나오는데 128장이 들어갔으니 128x128 이 나온다. 근데 -1 이니까 맨마지막셀을 말한다! >> 맨마지막셀을 뽑아냄\n",
    " ### 메모적은거보자 \n",
    "model = tf.matmul(outputs, W) + b  # 128x128 (아웃풋), 128x10 (가중치), --> 128x10\n",
    "cost = tf.reduce_mean(   \n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차수 돌아가는 것만을 잘 이해하면된다!! 중요 !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Avg. cost = 0.551\n",
      "Epoch: 0002 Avg. cost = 0.234\n",
      "Epoch: 0003 Avg. cost = 0.174\n",
      "Epoch: 0004 Avg. cost = 0.150\n",
      "Epoch: 0005 Avg. cost = 0.135\n",
      "Epoch: 0006 Avg. cost = 0.125\n",
      "Epoch: 0007 Avg. cost = 0.114\n",
      "Epoch: 0008 Avg. cost = 0.111\n",
      "Epoch: 0009 Avg. cost = 0.099\n",
      "Epoch: 0010 Avg. cost = 0.101\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "total_batch = int(mnist.train.num_examples/batch_size)  # 60000/ 128 로 나눴다 . \n",
    "for epoch in range(total_epoch):  # 10번돌아가면서 학습해라 \n",
    "    total_cost = 0\n",
    "    for i in range(total_batch):  #epoch 1번할때마다 전체사이즈를 다 돌리겠다 \n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)  # 원래데이터로 복원해야하니까 이미지사이즈로 리세잎한다 \n",
    "        batch_xs = batch_xs.reshape((batch_size, n_step, n_input))\n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X:batch_xs, Y:batch_ys})  # 언더바의 의미는 파이썬이 계산한 마지막 결과가 저장되는 것이다 > \n",
    "        total_cost += cost_val  # \n",
    "    print(\"Epoch:\", \"%04d\" % (epoch +1),\n",
    "          'Avg. cost =', '{:.3f}'.format(total_cost / total_batch))\n",
    "print(\"최적화 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://blog.naver.com/PostView.nhn?blogId=magnking&logNo=221323257045&redirect=Dlog&widgetTypeCall=true&directAccess=false  (이거보자!!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding/ one-hot-encoding/ cbow/ ngram   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 뉴런 : 뽑아내는 특징수 = hidden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제\n",
    "# 1. 테스트 데이터를 이용하여 테스트 하는 회로를 구성하시오\n",
    "# 2. 3개의 셀을 갖는 multi-layer cell 로 수정하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Variable:0\", shape=(128, 10), dtype=float32_ref) must be from the same graph as Tensor(\"strided_slice:0\", shape=(?, 128), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-ed1eb9f15701>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mmulti_layer_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   2567\u001b[0m       \u001b[0mare\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2568\u001b[0m   \"\"\"\n\u001b[1;32m-> 2569\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0madjoint_a\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2571\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Only one of transpose_a and adjoint_a can be True.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   6506\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6507\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6508\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6509\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6510\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   6133\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6134\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6135\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6136\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6137\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   6069\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6070\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" %\n\u001b[1;32m-> 6071\u001b[1;33m                      (item, original_item))\n\u001b[0m\u001b[0;32m   6072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"Variable:0\", shape=(128, 10), dtype=float32_ref) must be from the same graph as Tensor(\"strided_slice:0\", shape=(?, 128), dtype=float32)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "learning_rate = 0.001\n",
    "total_epoch = 10\n",
    "batch_size = 128\n",
    "n_input = 28\n",
    "n_step = 28\n",
    "n_hidden = 128\n",
    "n_class = 10\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_input]) # 128X28X28\n",
    "Y = tf.placeholder(tf.float32, [None, n_class]) # 128X10\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_class])) #128X10\n",
    "b = tf.Variable(tf.random_normal([n_class])) # 10\n",
    "\n",
    "n_layers = 3\n",
    "layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_hidden, activation=tf.nn.relu) for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "model = tf.matmul(outputs, W) + b\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "outputs = tf.transpose(outputs, [1,0,2])\n",
    "outputs = outputs[-1]\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "test_batch_size = len(mnist.test.images)\n",
    "test_xs = mnist.test.images.reshape(test_batch_size, n_step, n_input)\n",
    "test_ys = mnist.test.labels\n",
    "print('정확도: ', sess.run(accuracy, feed_dict={X: test_xs, Y: test_ys}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ft 가 forget 망 (이전에 들어오는 입력데어터 xt 와 이전의 output 즉 hidden 을 고려해서 가중치와 학습한 것을 control 선으로 제어 )???\n",
    "\n",
    "# 이전값을 고려한 값 과 입력데이터와 control 값(?) 도 고려하는 것이 LSTM   \n",
    "# 한 셀마다 가중치 4개가 필요하다 \n",
    "#앞에서는  output 되는 놈이 옆으로 나갔는데(1개) lSTM 에서는 state 망이 2개이다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력데이터의 shape (2, 3, 1)\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000015A7991CD08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000015A7991CD08>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000015A7991CD08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000015A7991CD08>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "(2, 100)\n",
      "(2, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "values231 = np.array([\n",
    "    [[1],[2],[3]],[[2],[3],[4]]\n",
    "])\n",
    "print('입력데이터의 shape', values231.shape)  # 2. 들어오는 데이터는 2x3x1 이다 (데이터 1개들어오고 100개가 나온다는것!!! )\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "tf.values231 = tf.constant(values231, dtype=tf.float32)\n",
    "#3. 하나의 가중치는 1x100 이 되고 가중치는 4개라 했으니 1x100 이 4개가 있다 \n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(num_units=100)  #1. >>> 뉴론이 100이라는것>> output이 100이라는것!!\n",
    "\n",
    "# 4. RNN 모델은 동일하다 \n",
    "outputs, state =tf.nn.dynamic_rnn(cell=lstm_cell, dtype=tf.float32, inputs=tf.values231) \n",
    "##print('rnn 이 출력하는 outputs 의 의미', outputs.shape)  # 2x3x100 이 됨 (2x3x1 이였는데 한개가 100개가 됐기때문에 2x3x100 이 된다) (데이터2개, 셀3개라는것) \n",
    "# state 는 2개가 나온다고 했다 \n",
    "print(state.c.shape)# state 는 2(데이터2개)x 100(100개특징)  (control state)\n",
    "print(state.h.shape) # 2x100                (hidden state) \n",
    "\n",
    "# 망과망을 연결할때(=번역망같은것, 앞에는 한글이들어오고 뒤는 영어가들어온다) state.c.shape 와 state.h.shape 와 연결해야한다!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_run, state_run=sess.run([outputs,state])\n",
    "    \n",
    "    # 내부구조가 복잡하지만 코드는 간단하다 ( latent time =지연시간=앞에계산하고 다음계산  을 가지고 결과값을 돌려준다 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000015A20A1C908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000015A20A1C908>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000015A20A1C908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000015A20A1C908>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000015A20D60FC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000015A20D60FC8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000015A20D60FC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x0000015A20D60FC8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "(2, 3, 100)\n",
      "(2, 3, 100)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "values=tf.constant(np.array([\n",
    "    [[1],[2],[3]],\n",
    "    [[2],[3],[4]]\n",
    "]), dtype=tf.float32)\n",
    "lstm_cell_fw= tf.contrib.rnn.LSTMCell(100)\n",
    "lstm_cell_bw= tf.contrib.rnn.LSTMCell(100)\n",
    "#( Multi Cell : 셀에서 이루어지고, 양방향 lSTm 은 model )  # 양방향 : 오른쪽 왼쪽에서 오면서 특징을 추출\n",
    "# output 도 2개, state 도 2개가 나온다 \n",
    "(output_fw, output_bw), (output_state_fw, output_state_bw) = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_cell_fw,\n",
    "                                                                                            cell_bw=lstm_cell_bw,\n",
    "                                                                                            inputs=values,\n",
    "                                                                                            dtype=tf.float32)\n",
    "print(output_fw.shape)\n",
    "print(output_bw.shape)\n",
    "\n",
    "# 양방향도 해보고 단일망도 해보면서 좋은 것을 선택하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 4)\n",
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]]], dtype=float32)\n",
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A20598AC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A20598AC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A20598AC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000015A20598AC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "array([[[-0.19372775,  0.12553886],\n",
      "        [-0.30280364, -0.35042408],\n",
      "        [-0.05948506,  0.15689123],\n",
      "        [-0.5061409 ,  0.37735775],\n",
      "        [ 0.7344311 , -0.47566003]]], dtype=float32)\n",
      "(1, 5, 2)\n",
      "(1, 2)\n",
      "array([[ 0.7344311 , -0.47566003]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# embedding 하는 것 \n",
    "from tensorflow.contrib import rnn\n",
    "import pprint\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "pp = pprint.PrettyPrinter(indent=4) ## pprint : 데이터형태대로 찍기위해서 준비된 함수이다 \n",
    "sess=tf.InteractiveSession()  #세션을 열고\n",
    "h=[1,0,0,0]  # 단어들을 one-hot encoding 을 했다 ( 단어를 원핫인코딩=수치화했다 !) \n",
    "e=[0,1,0,0]\n",
    "l=[0,0,1,0]\n",
    "o=[0,0,0,1] \n",
    "with tf.variable_scope('five_sequences') as scope: # variable_scope>>>변수공유를 위해 지정 \n",
    "    hidden_size = 2  # 4개로 들어와서 2개의 특징으로 추출된다는뜼!!! (4개에서 2개의 특징으로 vector 화 된다)\n",
    "    \n",
    "    cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "    x_data = np.array([[h,e,l,l,o]], dtype=np.float32)  \n",
    "    #[h,e,l,l,o]\n",
    "    #>>[1,0,0,0]\n",
    "    #  [0,1,0,0]\n",
    "    #  [0,0,1,0]\n",
    "    # [0,0,1,0]\n",
    "    # [0,0,0,1]  가 됨  >> shape 는 5x4 가 됨   (밖에서 1개를 더 씌어놨기때문에 1x5x4 이다 ) >> 3차원으로 만들기위해서 한개를씌운것이다 \n",
    "    print(x_data.shape)  # 1x5x4\n",
    "    \n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32) #dynamic_rnn 에 집어넣었다 \n",
    "    # >> cell의 갯수 : 5\n",
    "    # 셀당 입력데이터는 4/  셀당 가중치는 4x2 \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())  \n",
    "    print(outputs.shape)##1x5x2\n",
    "    print(states.shape)## 1x2  ( 데이터 1개가 들어갔으니 1개가 나온다? )\n",
    "    pp.pprint(states.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_arr = ['a','b','c','d','e','f','g',\n",
    "           'h','i','j','k','l','m','n',\n",
    "           'o','p','q','r','s','t','u',\n",
    "           'v','w','x','y','z'] # 26자를 code 화 하려고 만들었다>> 학습시켜서 마지막자를 예측하려고 함\n",
    "\n",
    "num_dic = {n:i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "\n",
    "# rule base -> 자동툴이 있어서 프로그램 자동으로 짜줬다 >> 가중치만 가지고있으면 바로 숫자 조합으로 예측할 수 있다\n",
    "\n",
    "seq_data =['word','wood','deep','dive','cold','cool','load','love','kiss','kind']\n",
    "\n",
    "#10x3\n",
    "#10x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어를 원핫인코딩했다 \n",
    "#10x3x26  \n",
    "def make_batch(seq_data):  # make_batch : 자동으로 문자를 원핫인코딩\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    for seq in seq_data:                   # word 가 seq 으로 들어온다   \n",
    "        input = [num_dic[n] for n in seq[:-1]] # seq[:-1 ]  마지막 글자까지  >> w 가  n 으로 들어간다 >  # num_dic[] 에 의해서 22번이 input 이 된다 \n",
    "                     # 하나뺐으니까 o 가 들어간다 >> o 는 14번, 그 다음꺼는 r (r은 17번)  마지막꺼는 안한다(-1이니까)\n",
    "        target = num_dic[seq[-1]] # target 에서 d=3 이라고 나온다 \n",
    "        input_batch.append(np.eye(dic_len)[input])  #dic_len>>  a : 0  ( 키로 즉 딕션으로 만들었으니까 저렇게 표현한다. r글자에 해당하는 번호로바뀜)\n",
    "                                                    # np.eye 에의해 >> a 는 100000000000000000000( 26글자로 표시)\n",
    "                                                                       #b는 010000000000000000000 으로 되고 결국 26x26 의 행렬이 된다 \n",
    "                                          # 22번째가 1인 놈, 14번이 0인놈, 17번이 0인 놈으로  구성 (one-hot encoding) >> 인풋배치에는 26x26 중 3개가 들어감\n",
    "                    \n",
    "        target_batch.append(target)\n",
    "        \n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate =0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 30   # 30번 돌아간다\n",
    "n_step = 3 \n",
    "n_input = n_class =dic_len  # 전부 26개다 \n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_input])  # 들어오는 데이터가 10x3x26 으로 들어온다 \n",
    "Y = tf.placeholder(tf.int32, [None])  # Y 는 10x26 으로 들어온다  \n",
    "#FFNN\n",
    "W = tf.Variable(tf.random_normal([n_hidden,n_class])) # 128 x 26\n",
    "b = tf.Variable(tf.random_normal([n_class])) # 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-80-d3c27701b18b>:1: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000015A2024AB48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000015A2024AB48>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000015A2024AB48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000015A2024AB48>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000015A24969248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000015A24969248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000015A24969248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000015A24969248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000015A72FE7448>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000015A72FE7448>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000015A72FE7448>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000015A72FE7448>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden) # n_hidden 을 basicLSTMCEll 에 줬으니까 특성수는 128이 된다 \n",
    "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob = 0.5, seed=10) # dropoutwrapper >> 윗줄에서 계산한양이 많으니까 줄여준다 # 50퍼센트만 남긴다 ( 0.5) >> 과적합을 방지하기 위해서 dropout 을 한다 \n",
    "\n",
    "cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])  # 2개의 layer 로 구성된 멀티셀을 만든다 \n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell,X, dtype=tf.float32)  # 다이내믹 rnn 에 집어넣는다 >> state 와 output 이 나온다 \n",
    "                                                                        # output 으로 나오는 놈 >> 10x3x128\n",
    "                                                                      # state 로 나오는 놈들 >> 10x128 개로 나온다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 플라스크 ,장고, 스프링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.transpose(outputs, [1,0,2]) # 3x10x 128 이 된다 >> 맨 뒷장만 뺌\n",
    "outputs = outputs[-1]    # 맨뒤의 셀에서 나온 10x128 만 남는다 \n",
    "model = tf.matmul(outputs, W) +b      # 비선형 회귀방식으로 문제를 풀고있다  \n",
    "\n",
    "# 분류에서 entropy 식이다 \n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=Y)) #\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 3.684258\n",
      "Epoch: 0002 cost= 2.680031\n",
      "Epoch: 0003 cost= 1.442163\n",
      "Epoch: 0004 cost= 1.480043\n",
      "Epoch: 0005 cost= 0.848839\n",
      "Epoch: 0006 cost= 0.619965\n",
      "Epoch: 0007 cost= 0.623532\n",
      "Epoch: 0008 cost= 0.478043\n",
      "Epoch: 0009 cost= 0.357932\n",
      "Epoch: 0010 cost= 0.303681\n",
      "Epoch: 0011 cost= 0.355491\n",
      "Epoch: 0012 cost= 0.408409\n",
      "Epoch: 0013 cost= 0.297510\n",
      "Epoch: 0014 cost= 0.238886\n",
      "Epoch: 0015 cost= 0.180458\n",
      "Epoch: 0016 cost= 0.152310\n",
      "Epoch: 0017 cost= 0.196188\n",
      "Epoch: 0018 cost= 0.079447\n",
      "Epoch: 0019 cost= 0.162526\n",
      "Epoch: 0020 cost= 0.087679\n",
      "Epoch: 0021 cost= 0.089254\n",
      "Epoch: 0022 cost= 0.102904\n",
      "Epoch: 0023 cost= 0.105722\n",
      "Epoch: 0024 cost= 0.036963\n",
      "Epoch: 0025 cost= 0.096502\n",
      "Epoch: 0026 cost= 0.079903\n",
      "Epoch: 0027 cost= 0.037834\n",
      "Epoch: 0028 cost= 0.022751\n",
      "Epoch: 0029 cost= 0.020562\n",
      "Epoch: 0030 cost= 0.020706\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "input_batch, target_batch = make_batch(seq_data)  #seq_data 는 단어데이터  \n",
    "\n",
    "for epoch in range(total_epoch):               # input batch 와 target-batch 학습하고 있다 \n",
    "    _, loss = sess.run([optimizer,cost],         \n",
    "                       feed_dict = {X: input_batch, Y: target_batch})\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1),    \n",
    "          'cost=','{:6f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.cast(tf.argmax(model,1), tf.int32)\n",
    "prediction_check = tf.equal(prediction,Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32))\n",
    "\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "predict, accuracy_val = sess.run([prediction, accuracy], feed_dict={X: input_batch, Y: target_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 문제 : predict 한 결과를 출력하고, accuracy 를  출력하시오 \n",
    "# 입력단어와 예측 결과를 나란히 출력해보시오 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
