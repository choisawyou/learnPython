{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "[cnn에서 중요한것은 망의 깊이 ]- 얼마나 깊게하는지 - 어떻게 깊게하는지 \n",
    "[cnn에서  중요한것은 이미지를 어떻게 늘릴것인지]\n",
    "\n",
    "\n",
    "[FILTER]\n",
    "- 필터사이즈가 크면 큰특징 작으면 작은특징\n",
    "    - 크게하면 손실이 크다 작게하면 deep하게 할 수 있따 \n",
    "        - 작게한 것 중 deep한것의 대표적인것은 VGG16,19\n",
    "        - identity 시키는 것_ 이전의 것을 고려 _ 깊게하더라도 살아남는다\n",
    "- 인셉션: 망을 깊게하기전에 수평으로한다 . \n",
    "\n",
    "[IMAGE]\n",
    "- image data generator:이미지 증강 \n",
    "    - 이미지 증강을 통해 이미지 구조문제 해결 : 이미지를 다양하게 변화\n",
    "        - pool , scale 등의 방법으로 \n",
    "        - 증강해논애들을 다시 save할 수 있따\n",
    "    - 파일로 많은 데이터를 저장해놨다가 읽어들이면서 fitting\n",
    " \n",
    "[MODEL]\n",
    "- 모델만들기 => compile => fit => evaluATE => PREDICT\n",
    "- 모델만드는 법 3 가지 :\n",
    "    - sequential \n",
    "    - functional : multi => multi\n",
    "        - 이미지가 특성이 다를떄?\n",
    "    - model:class 베이스로 모델을 구성하고 싶을때\n",
    "    \n",
    "- STATIC MODE:모델만들고 실행해야 결과알수있음:\n",
    "    - 속도가 아주빠르다\n",
    "- DIRECT MODE\n",
    "    - 효율성이 높다 \n",
    "    \n",
    "    \n",
    "    \n",
    "#현재 effcient net 에서 transfer learning으로 대세가 바뀌고 있따\n",
    "\n",
    "\n",
    "#http://incredible.ai/artificial-intelligence/2017/05/13/Transfer-Learning/\n",
    "- application : transfer learning: \n",
    "    - 기존의 만들어진 모델을 사용하여 새로운 모델을 만들시 학습을 빠르게 하며,\n",
    "      예측을 더 높이는 방법입니다.\n",
    "    - a를 b에 맵핑해준다?\n",
    "    - 데이터 부족문제 해결\n",
    "    ↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
    "    얘랑 의미가 비슷하다\n",
    "    cf)준지도학습: 라벨이 있는애와 없는애가 있따 : 라벨이 있는애를 기반으로 없는애한테 라벨을 달아준다 \n",
    "    - vert에도 적용 : pre training을 이용해 목적에 맞게 fine tuning한다 \n",
    "    \n",
    "- global average(maximum) pooling : \n",
    "    - covolution 연산이 끝난 후 flatten 해줄때 이미지의 특성을 잃지않게하는것?\n",
    "\n",
    "    \n",
    "- efficient net: 이미지를 잘처리하는 방법 : 동영상 처리에 부적합\n",
    "    - 1 filter의 사이즈 \n",
    "    - 2 depth:layer의 깊이\n",
    "    - 3 원본의 resolution: 원본의 해상도를 어떻게하느냐\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개념\n",
    "\n",
    "- transfer learning\n",
    "실질적으로 Convolution network을 처음부터 학습시키는 일은 많지 않습니다. 대부분의 문제는 이미 학습된 모델을 사용해서 문제를 해결할 수 있습니다.\n",
    "복잡한 모델일수록 학습시키기 어렵습니다. 어떤 모델은 2주정도 걸릴수 있으며, 비싼 GPU 여러대를 사용하기도 합니다.\n",
    "layers의 갯수, activation, hyper parameters등등 고려해야 할 사항들이 많으며, 실질적으로 처음부터 학습시키려면 많은 시도가 필요합니다.\n",
    "결론적으로 이미 잘 훈련된 모델이 있고, 특히 해당 모델과 유사한 문제를 해결시 transfer learining을 사용합니다.\n",
    "\n",
    "\n",
    "\n",
    "- convolution 2d\n",
    "필터로 특징을 뽑아주는 컨볼루션(Convolution) 레이어\n",
    "케라스에서 제공되는 컨볼루션 레이어 종류에도 여러가지가 있으나 영상 처리에 주로 사용되는 Conv2D 레이어를 살펴보겠습니다. 레이어는 영상 인식에 주로 사용되며, 필터가 탑재되어 있습니다.\n",
    "\n",
    "    - Conv2D(32, (5, 5), padding='valid', input_shape=(28, 28, 1), activation='relu')\n",
    "    \n",
    "    \n",
    "첫번째 인자 : 컨볼루션 필터의 수 입니다.\n",
    "두번째 인자 : 컨볼루션 커널의 (행, 열) 입니다.\n",
    "padding : 경계 처리 방법을 정의합니다.\n",
    "‘valid’ : 유효한 영역만 출력이 됩니다. 따라서 출력 이미지 사이즈는 입력 사이즈보다 작습니다.\n",
    "‘same’ : 출력 이미지 사이즈가 입력 이미지 사이즈와 동일합니다.\n",
    "input_shape : 샘플 수를 제외한 입력 형태를 정의 합니다. 모델에서 첫 레이어일 때만 정의하면 됩니다.\n",
    "(행, 열, 채널 수)로 정의합니다. 흑백영상인 경우에는 채널이 1이고, 컬러(RGB)영상인 경우에는 채널을 3으로 설정합\n",
    "\n",
    "activation : 활성화 함수 설정합니다.\n",
    "‘linear’ : 디폴트 값, 입력뉴런과 가중치로 계산된 결과값이 그대로 출력으로 나옵니다.\n",
    "‘relu’ : rectifier 함수, 은익층에 주로 쓰입니다.\n",
    "‘sigmoid’ : 시그모이드 함수, 이진 분류 문제에서 출력층에 주로 쓰입니다.\n",
    "‘softmax’ : 소프트맥스 함수, 다중 클래스 분류 문제에서 출력층에 주로 쓰입니다.\n",
    "입력 형태는 다음과 같습니다.\n",
    "\n",
    "image_data_format이 ‘channels_first’인 경우 (샘플 수, 채널 수, 행, 열)로 이루어진 4D 텐서입니다.\n",
    "image_data_format이 ‘channels_last’인 경우 (샘플 수, 행, 열, 채널 수)로 이루어진 4D 텐서입니다.\n",
    "\n",
    "image_data_format 옵션은 “keras.json” 파일 안에 있는 설정입니다. 콘솔에서 “vi ~/.keras/keras.json”으로 keras.json 파일 내용을 변경할 수 있습니다.\n",
    "\n",
    "출력 형태는 다음과 같습니다.\n",
    "\n",
    "image_data_format이 ‘channels_first’인 경우 (샘플 수, 필터 수, 행, 열)로 이루어진 4D 텐서입니다.\n",
    "image_data_format이 ‘channels_last’인 경우 (샘플 수, 행, 열, 필터 수)로 이루어진 4D 텐서입니다.\n",
    "행과 열의 크기는 padding가 ‘same’인 경우에는 입력 형태의 행과 열의 크기가 동일합니다.\n",
    "간단한 예제로 컨볼루션 레이어와 필터에 대해서 알아보겠습니다. 입력 이미지는 채널 수가 1, 너비가 3 픽셀, 높이가 3 픽셀이고, 크기가 2 x 2인 필터가 하나인 경우를 레이어로 표시하면 다음과 같습니다. 단 image_data_format이 ‘channels_last’인 경우 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras cnn MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "img_rows, img_cols=28,28\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first') # chaccels_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],1,28,28)\n",
    "X_test = X_test.reshape(X_test.shape[0],1,28,28)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],28,28,1)\n",
    "X_test = X_test.reshape(X_test.shape[0],28,28,1)\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape : {}'.format(X_train.shape))\n",
    "print('X_train samples : {}'.format(X_train.shape[0]))\n",
    "print('X_test samples : {}'.format(X_test.shape[0]))\n",
    "\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test,num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# output : 32, (3,3) : 필터 사이즈\n",
    "# 28x28 => (128,26,26,32)\n",
    "# same(28,28) : 모서리 특징 포함\n",
    "model.add(Conv2D(32,(3,3),activation='relu', input_shape=(28,28,1)))\n",
    "# Conv2D(32, (3, 3), padding='valid', input_shape=(28, 28, 1), activation='relu')\n",
    "\n",
    "# (128,24,24,64)\n",
    "# (128,12,12,64)\n",
    "#들어올때 (128,26,26,32)\n",
    "# 나갈 때 (128,24,24,64)\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())# 12x12x64 => 1차원\n",
    "\n",
    "# FFNN , FC(fully-connected)\n",
    "model.add(Dense(128,activation='relu'))# 9216x128\n",
    "# # 128x128\n",
    "model.add(Dropout(0.5)) # #계산을 50%하지말란 뜻 _차원이 바뀌진않음\n",
    "model.add(Dense(10, activation='softmax'))# 128X128  * 128X10 = 128X10\n",
    "# 128x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "# epoch마다 accuracy를 저장\n",
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    # 오버라이딩(재정의)\n",
    "    def on_train_begin(self, logs={}): # 훈련시작시 이벤트\n",
    "        self.acc = []\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('accuracy'))\n",
    "        \n",
    "history = AccuracyHistory()\n",
    "\n",
    "# 트레인, test\n",
    "# train, validation_data\n",
    "hist = model.fit(X_train, Y_train, batch_size=batch_size,epochs=epochs, verbose=1, validation_data=(X_test,Y_test), callbacks=[history])\n",
    "score = model.evaluate(X_test,Y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.on_train_begin()\n",
    "model.history.history.keys()\n",
    "# dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = X_train[0,:,:,0]\n",
    "plt.imshow(image, cmap=plt.cm.Greys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,13), model.history.history.keys()['acc'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장/로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델저장 / 로딩\n",
    "#hHDFS (HADOOP FILE SYSTEM) :모델 구조 전체 저장\n",
    "#가중치 ,구조, OPTIMIZATION STAGE 등이 저장\n",
    "#JSON 으로 저장 구조 가중치를 별도로 저장 => WEBㅇㅔ서 TESNSORFLOW\n",
    "#\n",
    "\n",
    "model.save('model_mnist.h5')\n",
    "print('모델이 저장되었습니다')\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model_mnist.h5')\n",
    "print('ㅁ델이 로딩되었습니다')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 레이어 정보 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = model.layers[0]\n",
    "l2 = model.layers[1]\n",
    "print(l1.name)\n",
    "print(l1.input_shape)\n",
    "print(l1.activation)\n",
    "print(l1.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE DATA GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "from keras import backend as K\n",
    "\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],28,28,1)\n",
    "X_test = X_test.reshape(X_test.shape[0],28,28,1)\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "\n",
    "#datagen = ImageDataGenerator(horizontal_flip = True,vertical_flip = True)\n",
    "\n",
    "# 데이터 위아래,옆으로 이동 \n",
    "#datagen = ImageDataGenerator(width_shift_range = 0.2,height_shift_range =0.2)\n",
    "\n",
    "#이미지 백색화=> noise 제거 ( 변수간 상관도를 없앰 pca)\n",
    "datagen = ImageDataGenerator(zca_whitening =True)\n",
    "\n",
    "#datagen =ImageDataGenerator(rotation_range=90)\n",
    "\n",
    "\n",
    "datagen.fit(X_train)\n",
    "#augment\n",
    "for X_batch,y_batch in datagen.flow(X_train,y_train,batch_size =9,\n",
    "                                   save_to_dir = 'images',\n",
    "                                    save_prefix='aug',\n",
    "                                    save_format='png'):\n",
    "    for i in range(0,9):\n",
    "        pyplot.subplot(330+1+i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28,28),\n",
    "                     cmap = pyplot.get_cmap('gray'))\n",
    "        pyplot.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10 \n",
    "from keras.optimizers import SGD,Adam,RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout,Flatten, Activation\n",
    "#cifar10 : airplane automobile cat deer dog bird frog horse shpe truck 분류\n",
    "\n",
    "(X_train,Y_train),(X_test,Y_test) = cifar10.load_data()\n",
    "print('X_TRAIN.SHAPE :',X_train.shape)\n",
    "print(X_train.shape[0],'train samples')\n",
    "print(X_test.shape[0],'test samples')\n",
    "\n",
    "NB_CLASSES = 10\n",
    " \n",
    "# 원핫 인코딩\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /=255\n",
    "X_test /=255\n",
    "\n",
    "plt.imshow(X_train[5])\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "plt.imshow(X_train[6])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CHANNELS =3\n",
    "IMG_ROWS =32\n",
    "IMG_COLS =32\n",
    "BATCH_SIZE =128\n",
    "NB_EPOCH =40\n",
    "NB_CLASSES = 10\n",
    "VERBOSE =1\n",
    "VALIDATION_SPLIT =0.2\n",
    "OPTIM = RMSprop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESIDUAL 망의 영향\n",
    "\n",
    "# 32X32X3(컬러망) =>  32X32X32 인풋망이 없음( 출력만 정해주면됨)\n",
    "model = Sequential() # \n",
    "model.add(Conv2D(32,kernel_size =3,padding = 'same',\n",
    "                 input_shape=(IMG_ROWS,IMG_COLS,IMG_CHANNELS)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32,kernel_size =3,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64,kernel_size =3,padding = 'same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64,3,3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten()) # 1차원 배열로 바꿔주기 \n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))              \n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = OPTIM,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train,Y_train,batch_size = BATCH_SIZE,\n",
    "                   epochs = NB_EPOCH,validation_split = VALIDATION_SPLIT,\n",
    "                   verbose = VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#똑같은ㄷ ㅔ이터 random 순서\n",
    "datagen = ImageDataGenerator(featurewise_center = False,\n",
    "                            samplewise_center = False,\n",
    "                            featurewise_std_normalization = False,\n",
    "                            samplewise_std_normalization =False,\n",
    "                            zca_whitening = False,\n",
    "                            rotation_range=0,\n",
    "                            width_shift_range =0.1,\n",
    "                            height_shift_range =0.1,\n",
    "                            horizontal_flip =True,\n",
    "                            vertical_flip = False)\n",
    "                            \n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(datagen.flow(X_train,Y_train,batch_size = BATCH_SIZE),\n",
    "                    samples_per_epoch =X_train.shape[0],\n",
    "                   nb_epoch = NB_EPOCH,verbose = VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test,Y_test,batch_size = BATCH_SIZE,verbose = VERBOSE)\n",
    "\n",
    "print('\\nTEST score L',score[0])\n",
    "\n",
    "\n",
    "print('\\nTrain score L',score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json','w').write(model_json)\n",
    "model.save_weights('cifar10_weights.h5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('cifa10_architecture.jso.json','r')\n",
    "loaded_model_json = json_file.read()\n",
    "json.file.close()\n",
    "loaded_model = model_from_json(loded_model_json)\n",
    "loaded_model,load_weights('cifar19_weights.h5')\n",
    "print('Load model from disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test,Y_test,batch_size = BATCH_SIZE,verbose = VERBOSE)\n",
    "\n",
    "print('\\nTEST score L',score[0])\n",
    "\n",
    "\n",
    "print('\\nTrain score L',score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ACCURACY GRAPH를 출력\n",
    "# 이미지 CAT DOG 를 다운로드한 다음 위의 모델로 예측해보시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc = 'upper left')```\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "from keras.models import model_from_json\n",
    "img_names = ['cat.jpg','dog.jpg']\n",
    "imgs = [resize(imread(img_name),(32,32)).astype('float32')  for img_name in img_names]\n",
    "imgs = np.array(imgs)/255\n",
    "predictions = model.predict_classes(imgs)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전이 학습 (Transfer learning) 학습 Application\n",
    "\n",
    "- 반지도 학습(일부데이터의 라벨이 무 ):knn, tranductive SVM\n",
    "- 가중치 활용 그대로 적용\n",
    "- 가중치 중 일부만 활용\n",
    "- FFNN 부부만 학습해서 사용\n",
    "\n",
    "\n",
    "#pre trained , fine tuning(FFNN 부분만 domain knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Transfer learning 이란?]\n",
    "\n",
    "- 트랜스퍼 러닝이란 딥러닝을 feature extractor로만 사용하고 그렇게 추출한 피처를 가지고 다른 모델을 학습하는 것\n",
    "- 기존의 만들어진 모델을 사용하여 새로운 모델을 만들시 학습을 빠르게 하며, 예측을 더 높이는 방법\n",
    "- 일반적으로 VGG,ResNet,gooGleNet등 이미 이러한 사전에 학습이 완료된 모델(Pre-Training Model)을 가지고 우리가 원하는 학습에 미세 조정 즉, 작은변화를 이용하여 학습시키는 방법이 Transfer Learning이다. \n",
    "- 이미학습된 weight들을 transfer(전송)하여 자신의 model에 맞게 학습을 시키는 방법\n",
    "- 신경망의 이러한 재학습 과정을 세부 조정(fine-tuning)이라 부름\n",
    "- 실제로 CNN을 구축하는 경우 대부분 처음부터 (random initialization) 학습하지는 않는다. \n",
    "-  ImageNet과 같은 대형 데이터셋을 사용해서 pretrain된 ConvNet 을 사용한다. \n",
    "\n",
    "\n",
    "[Fine-tuning 방법]\n",
    "3가지 방법이 있다.\n",
    "1. Feature extraction\n",
    "2. pre-trained model을 모델 구조를 이용\n",
    "3. 다른 레이어를 고정시키고 일부분 layer를 조정\n",
    "\n",
    "[Transfer Learning의 종류?]\n",
    "-  inception (googlenet), ms의 resNet, mobilenet, VGG 등\n",
    "- Pre trained VGG Model은 ImageNet 기반으로 학습이 된 Model\n",
    "- Inception v3는 ImageNet이라는 데이터를 분류하는데 학습이 되어 있다.\n",
    "[출처] [인턴일지] Transfer Learning (전이 학습) 이란?|작성자 nicewoong\n",
    "\n",
    "#resnet50 는 랜덤의 그림을 학습한 패키지 \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "import numpy as np\n",
    "from keras.applications.resnet50 import resnet50\n",
    "from keras.applications import resnet50\n",
    "\n",
    "filename ='banana.jpg'\n",
    "original =load_img(filename,target_size =(224,224))\n",
    "print('PIL image size', original.size)\n",
    "plt.imshow(original)\n",
    "plt.show()\n",
    "numpy_image = img_to_array(original)\n",
    "plt.imshow(np.uint8(numpy_image))\n",
    "print('numpy array size',numpy_image.shape)\n",
    "#차원확대 : 여러장 처리\n",
    "# 1장(3차원 => 4차원으로 확장)\n",
    "image_batch = np.expand_dims(numpy_image,axis =0)\n",
    "print('image batch size :',image_batch.shape)\n",
    "#prepare the image for the resnet50 model\n",
    "preprocessed_image = resnet50.preprocess_input(image_batch.copy())\n",
    "#dense 를 이용해서 모델을 생성\n",
    "#라벨 => 확률 : ㅣ\n",
    "# SOFTMAX 확률(상위3개로 예측)\n",
    "\n",
    "resnet_model = resnet50.ResNet50(weights = 'imagenet')\n",
    "predictions = resnet_model.predict(preprocessed_image)\n",
    "label = decode_predictions(predictions)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = decode_predictions(predictions,top =3)\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG / fine tuning\n",
    "\n",
    "[Fine-tuning 방법]\n",
    "3가지 방법이 있다.\n",
    "1. Feature extraction\n",
    "2. pre-trained model을 모델 구조를 이용\n",
    "3. 다른 레이어를 고정시키고 일부분 layer를 조정\n",
    "\n",
    "\n",
    "[VGG16]\n",
    "- https://kr.mathworks.com/help/deeplearning/ref/vgg16.html\n",
    "-  VGGNet은 16개 또는 19개의 층으로 구성된 모델을 의미한다(VGG16, VGG19로 불림). \n",
    " \n",
    "- VGG-16은 ImageNet 데이터베이스의 1백만 개가 넘는 이미지에 대해 훈련된 컨벌루션 신경망입니다[1]. 이 네트워크에는 16개의 계층이 있으며, 이미지를 키보드, 마우스, 연필, 각종 동물 등 1,000가지 사물 범주로 분류할 수 있습니다. 그 결과 이 네트워크는 다양한 이미지를 대표하는 다양한 특징을 학습했습니다. 네트워크의 이미지 입력 크기는 224x224입니다. MATLAB®의 여타 훈련된 네트워크에 대한 자세한 내용은 사전 훈련된 심층 신경망 항목을 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import randn\n",
    "import pathlib\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib.image import imread\n",
    "from keras.preprocessing import image\n",
    "# tf.enable_eager_execution()  -> 2.0 이 아닌경우 맨처음에 실행해줘야한다\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = 'C:/Users/ICT01_22/Documents/seoy/전달/flowers/flower_photos'\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "label_names= {'daisy':0, 'dandelion':1, 'roses':2, 'sunflowers':3, 'tulips':4}\n",
    "label_key = ['daisy','dandelion','roses','sunflowers','tulips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = list(data_dir.glob(\"*/*\"))\n",
    "all_images = [str(path) for path in all_images]\n",
    "random.shuffle(all_images)\n",
    "\n",
    "all_labels = [label_names[pathlib.Path(path).parent.name] for path in all_images]\n",
    "data_size = len(all_images)\n",
    "train_test_split=(int)(data_size*0.2)\n",
    "x_train=all_images[train_test_split:]\n",
    "x_test=all_images[:train_test_split]\n",
    "y_train=all_labels[train_test_split:]\n",
    "y_test = all_labels[:train_test_split]\n",
    "IMG_SIZE=160\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_data(x,y):\n",
    "    image = tf.read_file(x)\n",
    "    image = tf.image.decode_jpeg(image, channels= 3)\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    image = (image/127.5) -1 \n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image,y \n",
    "def _input_fn(x,y):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    ds = ds.map(_parse_data)\n",
    "    ds = ds.shuffle(buffer_size=data_size)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "train_ds = _input_fn(x_train,y_train)\n",
    "validation_ds = _input_fn(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_data(x,y):\n",
    "    image = tf.read_file(x)\n",
    "    image=tf.image.decode_jpeg(image,channels =3)\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    image =(image/127.5)-1\n",
    "    image = tf.image.resize(image,(IMG_SIZE,IMG_SIZE))\n",
    "    return image,y\n",
    "\n",
    "def input_fn(x,y):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    ds=ds.map(_parse_data)\n",
    "    ds = ds.shuffle(buffer_size = data_size)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size = AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "train_ds = input_fn(x_train,y_train)\n",
    "\n",
    "validation_ds = input_fn(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ICT01_22\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)  \n",
    "VGG16_MODEL = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
    "                                          include_top=False, weights='imagenet')\n",
    "\n",
    "\n",
    "#include_top=False : pooling 으로 1000개 분류하기 전에 모델셋팅 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLATTEN이 없고 이 기능을 하는것이  GlobalAveragePooling2D\n",
    "VGG16_MODEL.trainable =False\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(len(label_names),activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential([ VGG16_MODEL, global_average_layer, prediction_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "             loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Invalid JPEG data or crop window, data size 2636\n\t [[{{node DecodeJpeg}}]]\n\t [[IteratorGetNext]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-30fa9990e22f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m           \u001b[1;31m# `ins` can be callable in tf.distribute.Strategy + eager case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m           \u001b[0mactual_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Invalid JPEG data or crop window, data size 2636\n\t [[{{node DecodeJpeg}}]]\n\t [[IteratorGetNext]]"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=100, steps_per_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
